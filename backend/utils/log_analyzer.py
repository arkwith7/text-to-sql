"""
ë¡œê·¸ ë¶„ì„ ë° ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
utils/log_analyzer.py
"""

import json
import os
import gzip
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict, Counter
import argparse
import pandas as pd

class LogAnalyzer:
    """ë¡œê·¸ íŒŒì¼ ë¶„ì„ ë° ê´€ë¦¬ ë„êµ¬"""
    
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.log_files = {
            'app': self.log_dir / "app.log",
            'error': self.log_dir / "error.log",
            'api': self.log_dir / "api_requests.log",
            'sql': self.log_dir / "sql_queries.log",
            'chat': self.log_dir / "chat_sessions.log",
            'auth': self.log_dir / "authentication.log"
        }
    
    def parse_log_entry(self, line: str) -> Optional[Dict[str, Any]]:
        """JSON ë¡œê·¸ í•­ëª© íŒŒì‹±"""
        try:
            return json.loads(line.strip())
        except json.JSONDecodeError:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¡œê·¸ì¸ ê²½ìš°
            return {"message": line.strip(), "timestamp": None}
    
    def read_log_file(self, log_type: str, lines: int = None) -> List[Dict[str, Any]]:
        """ë¡œê·¸ íŒŒì¼ ì½ê¸°"""
        log_file = self.log_files.get(log_type)
        if not log_file or not log_file.exists():
            print(f"ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_file}")
            return []
        
        entries = []
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                log_lines = f.readlines()
                
                # ìµœê·¼ Nê°œ ë¼ì¸ë§Œ ì½ê¸°
                if lines:
                    log_lines = log_lines[-lines:]
                
                for line in log_lines:
                    entry = self.parse_log_entry(line)
                    if entry:
                        entries.append(entry)
        
        except Exception as e:
            print(f"ë¡œê·¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        
        return entries
    
    def analyze_errors(self, hours: int = 24) -> Dict[str, Any]:
        """ì—ëŸ¬ ë¡œê·¸ ë¶„ì„"""
        print(f"ğŸ“Š ìµœê·¼ {hours}ì‹œê°„ ì—ëŸ¬ ë¶„ì„ ì¤‘...")
        
        error_logs = self.read_log_file('error')
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        recent_errors = []
        error_counts = Counter()
        error_types = defaultdict(list)
        
        for entry in error_logs:
            try:
                log_time = datetime.fromisoformat(entry.get('timestamp', ''))
                if log_time >= cutoff_time:
                    recent_errors.append(entry)
                    
                    # ì—ëŸ¬ íƒ€ì…ë³„ ë¶„ë¥˜
                    error_msg = entry.get('message', '')
                    module = entry.get('module', 'unknown')
                    
                    error_counts[module] += 1
                    error_types[module].append({
                        'message': error_msg,
                        'timestamp': entry.get('timestamp'),
                        'function': entry.get('function')
                    })
            except:
                continue
        
        return {
            'total_errors': len(recent_errors),
            'error_by_module': dict(error_counts),
            'error_details': dict(error_types),
            'analysis_period': f"{hours} hours"
        }
    
    def analyze_api_performance(self, hours: int = 24) -> Dict[str, Any]:
        """API ì„±ëŠ¥ ë¶„ì„"""
        print(f"ğŸš€ ìµœê·¼ {hours}ì‹œê°„ API ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        
        api_logs = self.read_log_file('api')
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        response_times = []
        status_codes = Counter()
        endpoint_performance = defaultdict(list)
        slow_requests = []
        
        for entry in api_logs:
            try:
                log_time = datetime.fromisoformat(entry.get('timestamp', ''))
                if log_time >= cutoff_time and entry.get('event_type') == 'api_response':
                    response_time = entry.get('response_time_ms', 0)
                    status_code = entry.get('status_code')
                    path = entry.get('request_path', 'unknown')
                    
                    response_times.append(response_time)
                    status_codes[status_code] += 1
                    endpoint_performance[path].append(response_time)
                    
                    # ëŠë¦° ìš”ì²­ (2ì´ˆ ì´ìƒ)
                    if response_time > 2000:
                        slow_requests.append({
                            'path': path,
                            'response_time': response_time,
                            'timestamp': entry.get('timestamp'),
                            'status_code': status_code
                        })
            except:
                continue
        
        # ì—”ë“œí¬ì¸íŠ¸ë³„ í‰ê·  ì‘ë‹µì‹œê°„
        avg_response_times = {}
        for endpoint, times in endpoint_performance.items():
            avg_response_times[endpoint] = {
                'avg_ms': sum(times) / len(times) if times else 0,
                'max_ms': max(times) if times else 0,
                'min_ms': min(times) if times else 0,
                'request_count': len(times)
            }
        
        return {
            'total_requests': len(response_times),
            'avg_response_time_ms': sum(response_times) / len(response_times) if response_times else 0,
            'max_response_time_ms': max(response_times) if response_times else 0,
            'status_code_distribution': dict(status_codes),
            'endpoint_performance': avg_response_times,
            'slow_requests': slow_requests[:10],  # ìƒìœ„ 10ê°œ
            'analysis_period': f"{hours} hours"
        }
    
    def analyze_sql_queries(self, hours: int = 24) -> Dict[str, Any]:
        """SQL ì¿¼ë¦¬ ë¶„ì„"""
        print(f"ğŸ—„ï¸ ìµœê·¼ {hours}ì‹œê°„ SQL ì¿¼ë¦¬ ë¶„ì„ ì¤‘...")
        
        sql_logs = self.read_log_file('sql')
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        query_times = []
        slow_queries = []
        failed_queries = []
        query_patterns = Counter()
        
        for entry in sql_logs:
            try:
                log_time = datetime.fromisoformat(entry.get('timestamp', ''))
                if log_time >= cutoff_time and entry.get('event_type') == 'sql_execution':
                    execution_time = entry.get('execution_time', 0)
                    success = entry.get('success', True)
                    sql_query = entry.get('sql_query', '')
                    
                    if success:
                        query_times.append(execution_time)
                        
                        # ëŠë¦° ì¿¼ë¦¬ (1ì´ˆ ì´ìƒ)
                        if execution_time > 1.0:
                            slow_queries.append({
                                'query': sql_query[:100] + '...' if len(sql_query) > 100 else sql_query,
                                'execution_time': execution_time,
                                'timestamp': entry.get('timestamp'),
                                'user_id': entry.get('user_id')
                            })
                        
                        # ì¿¼ë¦¬ íŒ¨í„´ ë¶„ì„
                        query_type = self._extract_query_type(sql_query)
                        query_patterns[query_type] += 1
                    else:
                        failed_queries.append({
                            'query': sql_query[:100] + '...' if len(sql_query) > 100 else sql_query,
                            'error': entry.get('error_message'),
                            'timestamp': entry.get('timestamp'),
                            'user_id': entry.get('user_id')
                        })
            except:
                continue
        
        return {
            'total_queries': len(query_times) + len(failed_queries),
            'successful_queries': len(query_times),
            'failed_queries': len(failed_queries),
            'avg_execution_time_s': sum(query_times) / len(query_times) if query_times else 0,
            'max_execution_time_s': max(query_times) if query_times else 0,
            'slow_queries': slow_queries[:10],
            'failed_queries': failed_queries[:10],
            'query_type_distribution': dict(query_patterns),
            'analysis_period': f"{hours} hours"
        }
    
    def analyze_user_activity(self, hours: int = 24) -> Dict[str, Any]:
        """ì‚¬ìš©ì í™œë™ ë¶„ì„"""
        print(f"ğŸ‘¥ ìµœê·¼ {hours}ì‹œê°„ ì‚¬ìš©ì í™œë™ ë¶„ì„ ì¤‘...")
        
        auth_logs = self.read_log_file('auth')
        chat_logs = self.read_log_file('chat')
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        user_activity = defaultdict(lambda: {
            'login_count': 0,
            'chat_messages': 0,
            'last_activity': None
        })
        
        # ì¸ì¦ ë¡œê·¸ ë¶„ì„
        for entry in auth_logs:
            try:
                log_time = datetime.fromisoformat(entry.get('timestamp', ''))
                if log_time >= cutoff_time:
                    user_id = entry.get('user_id')
                    event_type = entry.get('event_type', '')
                    
                    if user_id and 'login_success' in event_type:
                        user_activity[user_id]['login_count'] += 1
                        user_activity[user_id]['last_activity'] = entry.get('timestamp')
            except:
                continue
        
        # ì±„íŒ… ë¡œê·¸ ë¶„ì„
        for entry in chat_logs:
            try:
                log_time = datetime.fromisoformat(entry.get('timestamp', ''))
                if log_time >= cutoff_time:
                    user_id = entry.get('user_id')
                    event_type = entry.get('event_type', '')
                    
                    if user_id and 'chat_message' in event_type:
                        user_activity[user_id]['chat_messages'] += 1
                        if not user_activity[user_id]['last_activity'] or \
                           entry.get('timestamp') > user_activity[user_id]['last_activity']:
                            user_activity[user_id]['last_activity'] = entry.get('timestamp')
            except:
                continue
        
        # í™œì„± ì‚¬ìš©ì ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_users = sorted(
            user_activity.items(),
            key=lambda x: x[1]['chat_messages'] + x[1]['login_count'],
            reverse=True
        )
        
        return {
            'total_active_users': len(user_activity),
            'top_active_users': dict(sorted_users[:10]),
            'total_logins': sum(user['login_count'] for user in user_activity.values()),
            'total_chat_messages': sum(user['chat_messages'] for user in user_activity.values()),
            'analysis_period': f"{hours} hours"
        }
    
    def _extract_query_type(self, sql_query: str) -> str:
        """SQL ì¿¼ë¦¬ íƒ€ì… ì¶”ì¶œ"""
        query_upper = sql_query.upper().strip()
        if query_upper.startswith('SELECT'):
            if 'JOIN' in query_upper:
                return 'SELECT_WITH_JOIN'
            elif 'GROUP BY' in query_upper:
                return 'SELECT_WITH_GROUP'
            else:
                return 'SELECT_SIMPLE'
        elif query_upper.startswith('INSERT'):
            return 'INSERT'
        elif query_upper.startswith('UPDATE'):
            return 'UPDATE'
        elif query_upper.startswith('DELETE'):
            return 'DELETE'
        else:
            return 'OTHER'
    
    def generate_report(self, hours: int = 24, output_file: str = None) -> Dict[str, Any]:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"ğŸ“‹ ìµœê·¼ {hours}ì‹œê°„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        report = {
            'generated_at': datetime.now().isoformat(),
            'analysis_period_hours': hours,
            'error_analysis': self.analyze_errors(hours),
            'api_performance': self.analyze_api_performance(hours),
            'sql_analysis': self.analyze_sql_queries(hours),
            'user_activity': self.analyze_user_activity(hours)
        }
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"ë¦¬í¬íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
        
        return report
    
    def cleanup_old_logs(self, days: int = 30):
        """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
        print(f"ğŸ§¹ {days}ì¼ ì´ì „ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        
        cutoff_date = datetime.now() - timedelta(days=days)
        cleaned_files = []
        
        for log_type, log_file in self.log_files.items():
            if not log_file.exists():
                continue
            
            # ë°±ì—… íŒŒì¼ë“¤ë„ ì •ë¦¬
            log_pattern = f"{log_file.stem}.*"
            for backup_file in self.log_dir.glob(log_pattern):
                try:
                    file_mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)
                    if file_mtime < cutoff_date:
                        if backup_file.suffix == '.gz':
                            backup_file.unlink()
                        else:
                            # ì••ì¶• í›„ ì‚­ì œ
                            with open(backup_file, 'rb') as f_in:
                                with gzip.open(f"{backup_file}.gz", 'wb') as f_out:
                                    shutil.copyfileobj(f_in, f_out)
                            backup_file.unlink()
                        
                        cleaned_files.append(str(backup_file))
                except Exception as e:
                    print(f"íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨ {backup_file}: {e}")
        
        print(f"ì •ë¦¬ëœ íŒŒì¼ ìˆ˜: {len(cleaned_files)}")
        return cleaned_files
    
    def tail_log(self, log_type: str, lines: int = 50):
        """ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ (tail -f ìœ ì‚¬)"""
        log_file = self.log_files.get(log_type)
        if not log_file or not log_file.exists():
            print(f"ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_file}")
            return
        
        print(f"ğŸ“¡ ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§: {log_file}")
        print("Ctrl+Cë¡œ ì¢…ë£Œí•˜ì„¸ìš”.\n")
        
        # ìµœê·¼ Nê°œ ë¼ì¸ ë¨¼ì € ì¶œë ¥
        recent_entries = self.read_log_file(log_type, lines)
        for entry in recent_entries:
            self._print_log_entry(entry)
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (ê°„ë‹¨í•œ êµ¬í˜„)
        try:
            import time
            with open(log_file, 'r', encoding='utf-8') as f:
                f.seek(0, 2)  # íŒŒì¼ ëìœ¼ë¡œ ì´ë™
                while True:
                    line = f.readline()
                    if line:
                        entry = self.parse_log_entry(line)
                        if entry:
                            self._print_log_entry(entry)
                    else:
                        time.sleep(0.1)
        except KeyboardInterrupt:
            print("\në¡œê·¸ ëª¨ë‹ˆí„°ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    
    def _print_log_entry(self, entry: Dict[str, Any]):
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ ì¶œë ¥"""
        timestamp = entry.get('timestamp', 'N/A')
        level = entry.get('level', 'INFO')
        message = entry.get('message', '')
        
        # ìƒ‰ìƒ ì½”ë“œ (í„°ë¯¸ë„ ì§€ì› ì‹œ)
        colors = {
            'ERROR': '\033[91m',  # ë¹¨ê°„ìƒ‰
            'WARNING': '\033[93m',  # ë…¸ë€ìƒ‰
            'INFO': '\033[92m',     # ì´ˆë¡ìƒ‰
            'DEBUG': '\033[94m',    # íŒŒë€ìƒ‰
            'RESET': '\033[0m'      # ë¦¬ì…‹
        }
        
        color = colors.get(level, colors['RESET'])
        reset = colors['RESET']
        
        print(f"{color}[{timestamp}] {level}: {message}{reset}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë¡œê·¸ ë¶„ì„ ë° ê´€ë¦¬ ë„êµ¬')
    parser.add_argument('--log-dir', default='logs', help='ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--hours', type=int, default=24, help='ë¶„ì„ ê¸°ê°„ (ì‹œê°„)')
    parser.add_argument('--report', action='store_true', help='ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±')
    parser.add_argument('--output', help='ë¦¬í¬íŠ¸ ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--cleanup', type=int, help='ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬ (ì¼ ìˆ˜)')
    parser.add_argument('--tail', choices=['app', 'error', 'api', 'sql', 'chat', 'auth'], 
                       help='ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§')
    parser.add_argument('--lines', type=int, default=50, help='tailì—ì„œ í‘œì‹œí•  ë¼ì¸ ìˆ˜')
    
    args = parser.parse_args()
    
    analyzer = LogAnalyzer(args.log_dir)
    
    if args.report:
        report = analyzer.generate_report(args.hours, args.output)
        if not args.output:
            # í„°ë¯¸ë„ì— ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
            print("\n=== ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸ ===")
            print(f"ë¶„ì„ ê¸°ê°„: {args.hours}ì‹œê°„")
            print(f"ì´ ì—ëŸ¬: {report['error_analysis']['total_errors']}ê°œ")
            print(f"ì´ API ìš”ì²­: {report['api_performance']['total_requests']}ê°œ")
            print(f"í‰ê·  API ì‘ë‹µì‹œê°„: {report['api_performance']['avg_response_time_ms']:.1f}ms")
            print(f"ì´ SQL ì¿¼ë¦¬: {report['sql_analysis']['total_queries']}ê°œ")
            print(f"í™œì„± ì‚¬ìš©ì: {report['user_activity']['total_active_users']}ëª…")
    
    elif args.cleanup:
        analyzer.cleanup_old_logs(args.cleanup)
    
    elif args.tail:
        analyzer.tail_log(args.tail, args.lines)
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()