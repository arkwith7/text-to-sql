"""
LangChain Agent for Text-to-SQL application.
Based on successful patterns from Jupyter notebook testing.
Uses latest LangChain APIs and patterns that have been proven to work.
"""

import logging
import time
from typing import Dict, Any, Optional, List, TYPE_CHECKING

from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import AzureChatOpenAI

# TYPE_CHECKINGì„ ì‚¬ìš©í•˜ì—¬ ìˆœí™˜ import ë°©ì§€
if TYPE_CHECKING:
    from database.connection_manager import DatabaseManager

from core.tools.langchain_tools import setup_langchain_tools, get_langchain_tools
from core.config import get_settings
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)

class LangChainTextToSQLAgent:
    """
    LangChain ê¸°ë°˜ Text-to-SQL Agent.
    ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ ì„±ê³µí•œ íŒ¨í„´ì„ ì ìš©í•˜ì—¬ êµ¬í˜„ë¨.
    """
    
    def __init__(
        self,
        db_manager: Optional["DatabaseManager"] = None,
        enable_simulation: bool = True,
        model_temperature: float = 0.1,
        max_iterations: int = 5,
        verbose: bool = True
    ):
        """
        LangChain Agent ì´ˆê¸°í™”.
        
        Args:
            db_manager: ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € (Optional, ìˆœí™˜ import ë°©ì§€)
            enable_simulation: ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ í™œì„±í™”
            model_temperature: LLM ì˜¨ë„ ì„¤ì •
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        """
        self.db_manager = db_manager
        self.enable_simulation = enable_simulation
        self.settings = get_settings()
        
        # ì„¤ì • ë¡œê¹…
        logger.info(f"LangChain Agent ì´ˆê¸°í™” ì‹œì‘")
        logger.info(f"  - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: {enable_simulation}")
        logger.info(f"  - ëª¨ë¸ ì˜¨ë„: {model_temperature}")
        logger.info(f"  - ìµœëŒ€ ë°˜ë³µ: {max_iterations}")
        logger.info(f"  - ìƒì„¸ ëª¨ë“œ: {verbose}")
        
        try:
            # Azure OpenAI LLM ì´ˆê¸°í™”
            self.llm = AzureChatOpenAI(
                azure_endpoint=self.settings.azure_openai_endpoint,
                api_key=self.settings.azure_openai_api_key,
                api_version=self.settings.azure_openai_api_version,
                azure_deployment=self.settings.azure_openai_deployment_name,
                temperature=model_temperature,
                max_tokens=2000
            )
            logger.info("âœ… Azure OpenAI LLM ì´ˆê¸°í™” ì™„ë£Œ")
            
            # LangChain Tools ì„¤ì • (ì§€ì—° import ë°©ì§€ë¥¼ ìœ„í•´ Noneìœ¼ë¡œ ì „ë‹¬)
            setup_langchain_tools(None, enable_simulation)
            self.tools = get_langchain_tools()
            logger.info(f"âœ… LangChain Tools ì´ˆê¸°í™” ì™„ë£Œ - {len(self.tools)}ê°œ ë„êµ¬")
            
            # System prompt ì •ì˜ (ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ ì„±ê³µí•œ íŒ¨í„´)
            self.system_prompt = """
ë‹¹ì‹ ì€ PostgreSQL Northwind ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ìì—°ì–´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ì‘ì—… ìˆœì„œ:
1. get_database_schema ë„êµ¬ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ í™•ì¸
2. generate_sql_from_question ë„êµ¬ë¡œ SQL ì¿¼ë¦¬ ìƒì„±  
3. execute_sql_query_sync ë„êµ¬ë¡œ ì¿¼ë¦¬ ì‹¤í–‰
4. ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…

ì£¼ì˜ì‚¬í•­:
- í•­ìƒ ìŠ¤í‚¤ë§ˆë¥¼ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”
- SQL ì¿¼ë¦¬ëŠ” PostgreSQL Northwind ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì •í™•í•´ì•¼ í•©ë‹ˆë‹¤
- í…Œì´ë¸”ëª…: customers, products, orders, categories, employees, suppliers, shippers, orderdetails
- ê²°ê³¼ëŠ” ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”
- ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ì›ì¸ì„ ë¶„ì„í•˜ê³  í•´ê²° ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”

PostgreSQL Northwind ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´:
- ê³ ê°(customers): 91ê°œ
- ì œí’ˆ(products): 77ê°œ  
- ì£¼ë¬¸(orders): 196ê°œ
- ì¹´í…Œê³ ë¦¬(categories): 8ê°œ
- ì§ì›(employees): 10ëª…
- ê³µê¸‰ì—…ì²´(suppliers): 29ê°œ
- ë°°ì†¡ì—…ì²´(shippers): 3ê°œ
"""
            
            # Chat prompt í…œí”Œë¦¿ ìƒì„±
            self.prompt = ChatPromptTemplate.from_messages([
                ("system", self.system_prompt),
                ("user", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ])
            
            # Agent ìƒì„± (ìµœì‹  API ì‚¬ìš©)
            self.agent = create_openai_functions_agent(
                self.llm, 
                self.tools, 
                self.prompt
            )
            
            # Agent Executor ìƒì„±
            self.agent_executor = AgentExecutor(
                agent=self.agent,
                tools=self.tools,
                verbose=verbose,
                handle_parsing_errors=True,
                max_iterations=max_iterations
            )
            
            logger.info("âœ… LangChain Agent ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"  - Agent Type: OpenAI Functions")
            logger.info(f"  - LLM Model: {self.settings.azure_openai_deployment_name}")
            logger.info(f"  - Available Tools: {len(self.tools)}ê°œ")
            
        except Exception as e:
            logger.error(f"âŒ LangChain Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def execute_query(
        self,
        question: str,
        user_id: Optional[str] = None,
        include_debug_info: bool = False
    ) -> Dict[str, Any]:
        """
        ìì—°ì–´ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ì—¬ SQL ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            question: ìì—°ì–´ ì§ˆë¬¸
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            include_debug_info: ë””ë²„ê·¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            logger.info(
                f"ğŸ¤– LangChain Agent ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘",
                extra={
                    'user_id': user_id,
                    'question': question[:100] + '...' if len(question) > 100 else question,
                    'include_debug': include_debug_info
                }
            )
            
            # Agent ì‹¤í–‰ (ìµœì‹  invoke ë©”ì„œë“œ ì‚¬ìš©)
            result = self.agent_executor.invoke({"input": question})
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ì²˜ë¦¬
            output = result.get('output', '')
            
            logger.info(
                f"âœ… LangChain Agent ì‹¤í–‰ ì™„ë£Œ - ì‹œê°„: {execution_time:.3f}s",
                extra={
                    'user_id': user_id,
                    'execution_time': execution_time,
                    'output_length': len(output),
                    'success': True
                }
            )
            
            response = {
                "success": True,
                "question": question,
                "answer": output,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation
            }
            
            if include_debug_info:
                response.update({
                    "debug_info": {
                        "intermediate_steps": result.get('intermediate_steps', []),
                        "tool_count": len(self.tools),
                        "max_iterations": self.agent_executor.max_iterations
                    }
                })
            
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            logger.error(
                f"âŒ LangChain Agent ì‹¤í–‰ ì‹¤íŒ¨ - ì‹œê°„: {execution_time:.3f}s",
                extra={
                    'user_id': user_id,
                    'execution_time': execution_time,
                    'error': error_msg,
                    'question': question[:100] + '...' if len(question) > 100 else question
                }
            )
            
            return {
                "success": False,
                "question": question,
                "error": error_msg,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation
            }
    
    async def execute_query_async(
        self,
        question: str,
        user_id: Optional[str] = None,
        include_debug_info: bool = False
    ) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸° ë²„ì „ì˜ ì¿¼ë¦¬ ì‹¤í–‰.
        
        Args:
            question: ìì—°ì–´ ì§ˆë¬¸
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            include_debug_info: ë””ë²„ê·¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ¤– ë¹„ë™ê¸° LangChain Agent ì‹¤í–‰ ì‹œì‘ - ì§ˆë¬¸: {question[:50]}...")
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            result = await self.agent_executor.ainvoke({"input": question})
            
            execution_time = time.time() - start_time
            output = result.get('output', '')
            
            logger.info(f"âœ… ë¹„ë™ê¸° ì‹¤í–‰ ì™„ë£Œ - ì‹œê°„: {execution_time:.3f}s")
            
            response = {
                "success": True,
                "question": question,
                "answer": output,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions_async",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation
            }
            
            if include_debug_info:
                response["debug_info"] = {
                    "intermediate_steps": result.get('intermediate_steps', []),
                    "tool_count": len(self.tools)
                }
            
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            logger.error(f"âŒ ë¹„ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}")
            
            return {
                "success": False,
                "question": question,
                "error": error_msg,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions_async"
            }
    
    def batch_execute(
        self,
        questions: List[str],
        user_id: Optional[str] = None,
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            questions: ì§ˆë¬¸ ëª©ë¡
            user_id: ì‚¬ìš©ì ID
            max_concurrent: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜
            
        Returns:
            ê²°ê³¼ ëª©ë¡
        """
        start_time = time.time()
        results = []
        
        logger.info(f"ğŸ“‹ ë°°ì¹˜ ì‹¤í–‰ ì‹œì‘ - {len(questions)}ê°œ ì§ˆë¬¸")
        
        for i, question in enumerate(questions, 1):
            logger.info(f"ğŸ”„ ë°°ì¹˜ ì§„í–‰: {i}/{len(questions)} - {question[:30]}...")
            
            result = self.execute_query(
                question=question,
                user_id=user_id,
                include_debug_info=False
            )
            
            # ë°°ì¹˜ ì •ë³´ ì¶”ê°€
            result["batch_info"] = {
                "batch_index": i,
                "total_questions": len(questions),
                "is_batch": True
            }
            
            results.append(result)
        
        total_time = time.time() - start_time
        successful_count = sum(1 for r in results if r.get('success', False))
        
        logger.info(
            f"âœ… ë°°ì¹˜ ì‹¤í–‰ ì™„ë£Œ - ì´ ì‹œê°„: {total_time:.3f}s, ì„±ê³µ: {successful_count}/{len(questions)}"
        )
        
        return results
    
    def get_agent_info(self) -> Dict[str, Any]:
        """
        Agent ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Agent ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "agent_type": "LangChain OpenAI Functions",
            "model": self.settings.azure_openai_deployment_name,
            "api_version": self.settings.azure_openai_api_version,
            "tools_count": len(self.tools),
            "tools": [tool.name for tool in self.tools],
            "simulation_mode": self.enable_simulation,
            "max_iterations": self.agent_executor.max_iterations,
            "database": "PostgreSQL Northwind",
            "supported_languages": ["Korean", "English"],
            "initialization_status": "ì™„ë£Œ"
        }
    
    def test_agent(self) -> Dict[str, Any]:
        """
        Agentì˜ ê¸°ë³¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        test_questions = [
            "ê³ ê° ìˆ˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
            "ì œí’ˆ ìˆ˜ëŠ” ëª‡ ê°œì¸ê°€ìš”?",
            "ì¹´í…Œê³ ë¦¬ë³„ ì œí’ˆ ìˆ˜ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
        ]
        
        logger.info("ğŸ§ª Agent ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_results = []
        start_time = time.time()
        
        for question in test_questions:
            result = self.execute_query(question, user_id="test_user", include_debug_info=False)
            test_results.append({
                "question": question,
                "success": result.get("success", False),
                "execution_time": result.get("execution_time", 0),
                "has_answer": bool(result.get("answer", "").strip())
            })
        
        total_time = time.time() - start_time
        success_rate = sum(1 for r in test_results if r["success"]) / len(test_results) * 100
        
        test_summary = {
            "test_completed": True,
            "total_tests": len(test_questions),
            "success_rate": f"{success_rate:.1f}%",
            "total_time": f"{total_time:.3f}ì´ˆ",
            "avg_time": f"{total_time/len(test_questions):.3f}ì´ˆ",
            "results": test_results,
            "agent_info": self.get_agent_info()
        }
        
        logger.info(f"âœ… Agent í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µë¥ : {success_rate:.1f}%")
        
        return test_summary 