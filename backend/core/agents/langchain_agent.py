"""
LangChain Agent for Text-to-SQL application.
Based on successful patterns from Jupyter notebook testing.
Uses latest LangChain APIs and patterns that have been proven to work.
Includes LLM token usage tracking functionality.
"""

import logging
import time
import uuid
from typing import Dict, Any, Optional, List, TYPE_CHECKING

from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import AzureChatOpenAI

# TYPE_CHECKINGì„ ì‚¬ìš©í•˜ì—¬ ìˆœí™˜ import ë°©ì§€
if TYPE_CHECKING:
    from database.connection_manager import DatabaseManager

from core.tools.langchain_tools import setup_langchain_tools, get_langchain_tools
from core.config import get_settings
from utils.logging_config import setup_logging
from services.token_usage_service import TokenUsageService

logger = logging.getLogger(__name__)

class LangChainTextToSQLAgent:
    """
    LangChain ê¸°ë°˜ Text-to-SQL Agent.
    ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ ì„±ê³µí•œ íŒ¨í„´ì„ ì ìš©í•˜ì—¬ êµ¬í˜„ë¨.
    """
    
    def __init__(
        self,
        db_manager: Optional["DatabaseManager"] = None,
        enable_simulation: bool = True,
        model_temperature: float = 0.1,
        max_iterations: int = 5,
        verbose: bool = True
    ):
        """
        LangChain Agent ì´ˆê¸°í™”.
        
        Args:
            db_manager: ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € (Optional, ìˆœí™˜ import ë°©ì§€)
            enable_simulation: ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ í™œì„±í™”
            model_temperature: LLM ì˜¨ë„ ì„¤ì •
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        """
        self.db_manager = db_manager
        self.enable_simulation = enable_simulation
        self.settings = get_settings()
        
        # Initialize token usage service (ì§€ì—° ë¡œë”©)
        self.db_manager = db_manager
        self.token_usage_service = None
        
        # ì„¤ì • ë¡œê¹…
        logger.info(f"LangChain Agent ì´ˆê¸°í™” ì‹œì‘")
        logger.info(f"  - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: {enable_simulation}")
        logger.info(f"  - ëª¨ë¸ ì˜¨ë„: {model_temperature}")
        logger.info(f"  - ìµœëŒ€ ë°˜ë³µ: {max_iterations}")
        logger.info(f"  - ìƒì„¸ ëª¨ë“œ: {verbose}")
        logger.info(f"  - í† í° ì¶”ì : {'í™œì„±í™”' if db_manager else 'ë¹„í™œì„±í™”'}")
        
        # Azure OpenAI LLM ì´ˆê¸°í™”
        self.llm = AzureChatOpenAI(
            azure_endpoint=self.settings.azure_openai_endpoint,
            api_key=self.settings.azure_openai_api_key,
            api_version=self.settings.azure_openai_api_version,
            azure_deployment=self.settings.azure_openai_deployment_name,
            temperature=model_temperature,
            max_tokens=2000
        )
        logger.info("âœ… Azure OpenAI LLM ì´ˆê¸°í™” ì™„ë£Œ")
        
        # LangChain Tools ì„¤ì • (ì§€ì—° import ë°©ì§€ë¥¼ ìœ„í•´ Noneìœ¼ë¡œ ì „ë‹¬)
        setup_langchain_tools(None, enable_simulation)
        self.tools = get_langchain_tools()
        logger.info(f"âœ… LangChain Tools ì´ˆê¸°í™” ì™„ë£Œ - {len(self.tools)}ê°œ ë„êµ¬")
        
        # System prompt ì •ì˜ (ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ ì„±ê³µí•œ íŒ¨í„´ + ë³´ì•ˆ ê°•í™”)
        self.system_prompt = """
ë‹¹ì‹ ì€ PostgreSQL Northwind ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ìì—°ì–´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ğŸ”’ ì¤‘ìš”í•œ ë³´ì•ˆ ê·œì¹™:
- ì´ ì‹œìŠ¤í…œì€ ì½ê¸° ì „ìš©(READ-ONLY)ì…ë‹ˆë‹¤
- SELECT ì¿¼ë¦¬ë§Œ í—ˆìš©ë©ë‹ˆë‹¤
- INSERT, UPDATE, DELETE, DROP, ALTER ë“±ì˜ ë°ì´í„° ë³€ê²½ ì‘ì—…ì€ ì ˆëŒ€ ê¸ˆì§€ë©ë‹ˆë‹¤
- ë°ì´í„°ë² ì´ìŠ¤ì˜ êµ¬ì¡°ë‚˜ ë°ì´í„°ë¥¼ ë³€ê²½í•˜ë ¤ëŠ” ì‹œë„ë¥¼ í•˜ì§€ ë§ˆì„¸ìš”

ì‘ì—… ìˆœì„œ:
1. get_database_schema ë„êµ¬ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ í™•ì¸
2. generate_sql_from_question ë„êµ¬ë¡œ SQL ì¿¼ë¦¬ ìƒì„±  
3. execute_sql_query_sync ë„êµ¬ë¡œ ì¿¼ë¦¬ ì‹¤í–‰
4. ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…
"""
        
        # Chat prompt í…œí”Œë¦¿ ìƒì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        # Agent ìƒì„± (ìµœì‹  API ì‚¬ìš©)
        self.agent = create_openai_functions_agent(
            self.llm, 
            self.tools, 
            prompt
        )
        
        # Agent Executor ìƒì„±
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=verbose,
            handle_parsing_errors=True,
            max_iterations=max_iterations,
            return_intermediate_steps=True  # ì¤‘ê°„ ë‹¨ê³„ ì •ë³´ ë°˜í™˜
        )
        
        logger.info("âœ… LangChain Agent ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_token_usage_service(self):
        """í† í° ì‚¬ìš©ëŸ‰ ì„œë¹„ìŠ¤ë¥¼ ì§€ì—° ë¡œë”©í•©ë‹ˆë‹¤."""
        if self.token_usage_service is None and self.db_manager:
            try:
                self.token_usage_service = TokenUsageService(self.db_manager)
                logger.info("âœ… LangChain Token Usage Service ì—°ë™ ì™„ë£Œ")
            except ImportError as e:
                logger.warning(f"Token Usage Service ë¡œë”© ì‹¤íŒ¨: {e}")
        return self.token_usage_service
    
    async def execute_query(
        self,
        question: str,
        database: str = "northwind",
        context: Optional[str] = None,
        user_id: Optional[str] = None,
        include_explanation: bool = True,
        include_debug_info: bool = False,
        max_rows: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ìì—°ì–´ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ì—¬ SQL ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            question: ìì—°ì–´ ì§ˆë¬¸
            database: ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ (ê¸°ë³¸ê°’: northwind)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            include_explanation: ì„¤ëª… í¬í•¨ ì—¬ë¶€
            include_debug_info: ë””ë²„ê·¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            max_rows: ìµœëŒ€ ë°˜í™˜ í–‰ ìˆ˜ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            logger.info(
                f"ğŸ¤– LangChain Agent ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘",
                extra={
                    'user_id': user_id,
                    'question': question[:100] + '...' if len(question) > 100 else question,
                    'database': database,
                    'include_debug': include_debug_info
                }
            )
            
            # Contextê°€ ìˆìœ¼ë©´ ì§ˆë¬¸ì— ì¶”ê°€
            enhanced_question = question
            if context:
                enhanced_question = f"{question}\n\nì¶”ê°€ ì¡°ê±´: {context}"
            
            # Azure OpenAI í˜¸ì¶œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•´ ì½œë°± ì‚¬ìš©
            try:
                from langchain_community.callbacks import get_openai_callback
                logger.info("âœ… LangChain Community ì½œë°± ë¡œë“œ ì„±ê³µ")
            except ImportError:
                try:
                    from langchain.callbacks import get_openai_callback
                    logger.info("âœ… LangChain Core ì½œë°± ë¡œë“œ ì„±ê³µ")
                except ImportError:
                    logger.warning("âš ï¸ OpenAI ì½œë°±ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    # ì½œë°± ì—†ì´ ì‹¤í–‰
                    result = await self.agent_executor.ainvoke({"input": enhanced_question})
                    execution_time = time.time() - start_time
                    output = result.get('output', '')
                    intermediate_steps = result.get('intermediate_steps', [])
                    
                    logger.info(f"ğŸ” Agent ì¶œë ¥ ê¸¸ì´: {len(output)} ë¬¸ì")
                    logger.info(f"ğŸ” Agent ì¶œë ¥ ë‚´ìš©: '{output[:100]}...' " if len(output) > 100 else f"'{output}'")
                    logger.info(f"ğŸ” ì¤‘ê°„ ë‹¨ê³„ ìˆ˜: {len(intermediate_steps)}")
                    
                    # Check if agent failed due to iteration/time limit and provide user-friendly message
                    is_agent_limit_error = (
                        "Agent stopped due to iteration limit or time limit" in output or
                        output.strip() == "Agent stopped due to iteration limit or time limit."
                    )
                    
                    if is_agent_limit_error:
                        logger.warning("âš ï¸ Agentê°€ iteration/time limitìœ¼ë¡œ ì¸í•´ ì¤‘ë‹¨ë¨ (ì½œë°± ì—†ìŒ)")
                        output = "ì§ˆë¬¸ì´ ë„ˆë¬´ ë³µì¡í•˜ê±°ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°ì— ë§ëŠ” ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”."
                    
                    # ì¤‘ê°„ ë‹¨ê³„ì—ì„œ SQL ì¿¼ë¦¬ì™€ ê²°ê³¼ ì¶”ì¶œ
                    executed_sql = None
                    sql_results = []
                    
                    for i, (agent_action, observation) in enumerate(intermediate_steps, 1):
                        observation_str = str(observation)
                        logger.info(f"ğŸ” ë‹¨ê³„ {i}: ë„êµ¬={agent_action.tool}, ê´€ì°°={observation_str[:200]}...")
                        
                        # execute_sql_query_sync ë„êµ¬ì˜ ê²°ê³¼ì—ì„œ SQLê³¼ ë°ì´í„° ì¶”ì¶œ
                        if agent_action.tool == "execute_sql_query_sync":
                            try:
                                # ê´€ì°°ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                                if isinstance(observation, dict):
                                    obs_dict = observation
                                elif isinstance(observation, str):
                                    # ë¬¸ìì—´ì´ë©´ JSON íŒŒì‹± ì‹œë„
                                    import json
                                    try:
                                        obs_dict = json.loads(observation)
                                    except json.JSONDecodeError:
                                        logger.warning(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {observation[:100]}...")
                                        continue
                                else:
                                    logger.warning(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ê´€ì°° íƒ€ì…: {type(observation)}")
                                    continue
                                
                                # SQL ì¿¼ë¦¬ì™€ ê²°ê³¼ ì¶”ì¶œ
                                if obs_dict.get('success'):
                                    if obs_dict.get('sql_query'):
                                        executed_sql = obs_dict.get('sql_query')
                                        sql_results = obs_dict.get('results', [])
                                        logger.info(f"ğŸ¯ SQL ì¿¼ë¦¬ ì¶”ì¶œ ì„±ê³µ: {executed_sql[:100]}...")
                                        logger.info(f"ğŸ¯ ê²°ê³¼ í–‰ ìˆ˜: {len(sql_results)}")
                                        break  # ì²« ë²ˆì§¸ ì„±ê³µí•œ SQL ì‹¤í–‰ ê²°ê³¼ ì‚¬ìš©
                                    else:
                                        logger.warning(f"âš ï¸ SQL ì¿¼ë¦¬ê°€ ì—†ìŒ: {obs_dict}")
                                else:
                                    logger.warning(f"âš ï¸ SQL ì‹¤í–‰ ì‹¤íŒ¨: {obs_dict}")
                            except Exception as e:
                                logger.error(f"âŒ SQL ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}, ê´€ì°°: {observation_str[:100]}...")
                    
                    # í† í° ì •ë³´ ì—†ì´ ì‘ë‹µ ìƒì„±
                    logger.info("âš ï¸ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ì—†ìŒ")
                    
                    response = {
                        "success": True,
                        "question": question,
                        "answer": output,
                        "sql_query": executed_sql or "",
                        "results": sql_results or [],
                        "execution_time": execution_time,
                        "agent_type": "langchain_openai_functions",
                        "model": self.settings.azure_openai_deployment_name,
                        "simulation_mode": self.enable_simulation,
                        "explanation": output  # Agentì˜ ë‹µë³€ì„ ì„¤ëª…ìœ¼ë¡œë„ ì‚¬ìš©
                    }
                    
                    if include_debug_info:
                        response.update({
                            "debug_info": {
                                "intermediate_steps": intermediate_steps,
                                "tool_count": len(self.tools),
                                "max_iterations": self.agent_executor.max_iterations,
                                "executed_sql": executed_sql,
                                "sql_results_count": len(sql_results) if sql_results else 0
                            }
                        })
                    
                    logger.info(
                        f"âœ… LangChain Agent ì‹¤í–‰ ì™„ë£Œ (ì½œë°± ì—†ìŒ) - ì‹œê°„: {execution_time:.3f}s",
                        extra={
                            'user_id': user_id,
                            'execution_time': execution_time,
                            'output_length': len(output),
                            'success': True
                        }
                    )
                    
                    return response
            
            logger.info("ğŸ” í† í° ì½œë°± ì‹œì‘ - Agent ì‹¤í–‰ ì „")
            
            with get_openai_callback() as cb:
                # Agent ì‹¤í–‰ (ë¹„ë™ê¸°ë¡œ ë³€ê²½)
                logger.info("ğŸ¤– Agent executor ainvoke ì‹œì‘")
                result = await self.agent_executor.ainvoke({"input": enhanced_question})
                logger.info(f"ğŸ¤– Agent executor ainvoke ì™„ë£Œ - ê²°ê³¼ í‚¤: {list(result.keys())}")
                
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ì²˜ë¦¬
                output = result.get('output', '')
                intermediate_steps = result.get('intermediate_steps', [])
                
                logger.info(f"ğŸ” Agent ì¶œë ¥ ê¸¸ì´: {len(output)} ë¬¸ì")
                logger.info(f"ğŸ” Agent ì¶œë ¥ ë‚´ìš©: '{output[:100]}...' " if len(output) > 100 else f"'{output}'")
                logger.info(f"ğŸ” ì¤‘ê°„ ë‹¨ê³„ ìˆ˜: {len(intermediate_steps)}")
                
                # Check if agent failed due to iteration/time limit and provide user-friendly message
                is_agent_limit_error = (
                    "Agent stopped due to iteration limit or time limit" in output or
                    output.strip() == "Agent stopped due to iteration limit or time limit."
                )
                
                if is_agent_limit_error:
                    logger.warning("âš ï¸ Agentê°€ iteration/time limitìœ¼ë¡œ ì¸í•´ ì¤‘ë‹¨ë¨")
                    output = "ì§ˆë¬¸ì´ ë„ˆë¬´ ë³µì¡í•˜ê±°ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°ì— ë§ëŠ” ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”."
                
                # ì¤‘ê°„ ë‹¨ê³„ì—ì„œ SQL ì¿¼ë¦¬ì™€ ê²°ê³¼ ì¶”ì¶œ
                executed_sql = None
                sql_results = []
                
                for i, (agent_action, observation) in enumerate(intermediate_steps, 1):
                    observation_str = str(observation)
                    logger.info(f"ğŸ” ë‹¨ê³„ {i}: ë„êµ¬={agent_action.tool}, ê´€ì°°={observation_str[:200]}...")
                    
                    # execute_sql_query_sync ë„êµ¬ì˜ ê²°ê³¼ì—ì„œ SQLê³¼ ë°ì´í„° ì¶”ì¶œ
                    if agent_action.tool == "execute_sql_query_sync":
                        try:
                            # ê´€ì°°ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                            if isinstance(observation, dict):
                                obs_dict = observation
                            elif isinstance(observation, str):
                                # ë¬¸ìì—´ì´ë©´ JSON íŒŒì‹± ì‹œë„
                                import json
                                try:
                                    obs_dict = json.loads(observation)
                                except json.JSONDecodeError:
                                    logger.warning(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {observation[:100]}...")
                                    continue
                            else:
                                logger.warning(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ê´€ì°° íƒ€ì…: {type(observation)}")
                                continue
                            
                            # SQL ì¿¼ë¦¬ì™€ ê²°ê³¼ ì¶”ì¶œ
                            if obs_dict.get('success'):
                                if obs_dict.get('sql_query'):
                                    executed_sql = obs_dict.get('sql_query')
                                    sql_results = obs_dict.get('results', [])
                                    logger.info(f"ğŸ¯ SQL ì¿¼ë¦¬ ì¶”ì¶œ ì„±ê³µ: {executed_sql[:100]}...")
                                    logger.info(f"ğŸ¯ ê²°ê³¼ í–‰ ìˆ˜: {len(sql_results)}")
                                    break  # ì²« ë²ˆì§¸ ì„±ê³µí•œ SQL ì‹¤í–‰ ê²°ê³¼ ì‚¬ìš©
                                else:
                                    logger.warning(f"âš ï¸ SQL ì¿¼ë¦¬ê°€ ì—†ìŒ: {obs_dict}")
                            else:
                                logger.warning(f"âš ï¸ SQL ì‹¤í–‰ ì‹¤íŒ¨: {obs_dict}")
                        except Exception as e:
                            logger.error(f"âŒ SQL ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}, ê´€ì°°: {observation_str[:100]}...")
                
                # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ ë° ê¸°ë¡
                logger.info(f"ğŸ” ì½œë°± ê°ì²´ íƒ€ì…: {type(cb)}")
                logger.info(f"ğŸ” ì½œë°± í† í° ì‚¬ìš©ëŸ‰: {cb.total_tokens} (prompt: {cb.prompt_tokens}, completion: {cb.completion_tokens})")
                logger.info(f"ğŸ” ì½œë°± successful_requests: {getattr(cb, 'successful_requests', 'N/A')}")
                logger.info(f"ğŸ” ì½œë°± total_cost: {getattr(cb, 'total_cost', 'N/A')}")
                logger.info(f"ğŸ” ì‚¬ìš©ì ID: {user_id}")
                
                if user_id and cb.total_tokens > 0:
                    token_usage = {
                        "prompt_tokens": cb.prompt_tokens,
                        "completion_tokens": cb.completion_tokens,
                        "total_tokens": cb.total_tokens
                    }
                    logger.info(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹œì‘: {token_usage}")
                    
                    # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ (ë¹„ë™ê¸° ë²„ì „)
                    try:
                        token_service = self._get_token_usage_service()
                        logger.info(f"ğŸ”§ í† í° ì„œë¹„ìŠ¤: {token_service is not None}")
                        
                        if token_service:
                            query_id = str(uuid.uuid4())
                            model_name = self.settings.azure_openai_deployment_name
                            
                            logger.info(f"ğŸ“ í† í° ê¸°ë¡ ì‹œì‘ - Query ID: {query_id}, Model: {model_name}")
                            
                            # ë¹„ë™ê¸° ë©”ì„œë“œ í˜¸ì¶œ
                            await token_service.record_token_usage(
                                user_id=user_id,
                                session_id="langchain_session",
                                message_id=query_id,
                                token_usage=token_usage,
                                model_name=model_name,
                                query_type="text_to_sql",
                                additional_metadata={
                                    "question": question,
                                    "agent_type": "langchain_openai_functions",
                                    "execution_time": execution_time
                                }
                            )
                            
                            logger.info(
                                f"ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì™„ë£Œ - ì´: {token_usage['total_tokens']}, "
                                f"ì…ë ¥: {token_usage['prompt_tokens']}, ì¶œë ¥: {token_usage['completion_tokens']}"
                            )
                    except Exception as token_error:
                        logger.error(f"í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹¤íŒ¨: {token_error}")
                else:
                    logger.warning(f"í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì¡°ê±´ ë¯¸ì¶©ì¡± - user_id: {user_id}, tokens: {cb.total_tokens if 'cb' in locals() else 'N/A'}")
            
            logger.info(
                f"âœ… LangChain Agent ì‹¤í–‰ ì™„ë£Œ - ì‹œê°„: {execution_time:.3f}s",
                extra={
                    'user_id': user_id,
                    'execution_time': execution_time,
                    'output_length': len(output),
                    'success': True
                }
            )
            
            response = {
                "success": True,
                "question": question,
                "answer": output,
                "sql_query": executed_sql or "",
                "results": sql_results or [],
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation,
                "explanation": output  # Agentì˜ ë‹µë³€ì„ ì„¤ëª…ìœ¼ë¡œë„ ì‚¬ìš©
            }
            
            if include_debug_info:
                response.update({
                    "debug_info": {
                        "intermediate_steps": intermediate_steps,
                        "tool_count": len(self.tools),
                        "max_iterations": self.agent_executor.max_iterations,
                        "executed_sql": executed_sql,
                        "sql_results_count": len(sql_results) if sql_results else 0
                    }
                })
            
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            logger.error(
                f"âŒ LangChain Agent ì‹¤í–‰ ì‹¤íŒ¨ - ì‹œê°„: {execution_time:.3f}s",
                extra={
                    'user_id': user_id,
                    'execution_time': execution_time,
                    'error': error_msg,
                    'question': question[:100] + '...' if len(question) > 100 else question
                }
            )
            
            return {
                "success": False,
                "question": question,
                "error": error_msg,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation
            }
    
    def execute_query_sync(
        self,
        question: str,
        database: str = "northwind",
        context: Optional[str] = None,
        user_id: Optional[str] = None,
        include_explanation: bool = True,
        include_debug_info: bool = False,
        max_rows: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ë™ê¸° ë²„ì „ì˜ ì¿¼ë¦¬ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬ìš©).
        
        Args:
            question: ìì—°ì–´ ì§ˆë¬¸
            database: ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ (ê¸°ë³¸ê°’: northwind)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            include_explanation: ì„¤ëª… í¬í•¨ ì—¬ë¶€
            include_debug_info: ë””ë²„ê·¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            max_rows: ìµœëŒ€ ë°˜í™˜ í–‰ ìˆ˜ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            logger.info(
                f"ğŸ¤– LangChain Agent ë™ê¸° ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘",
                extra={
                    'user_id': user_id,
                    'question': question[:100] + '...' if len(question) > 100 else question,
                    'database': database,
                    'include_debug': include_debug_info
                }
            )
            
            # Contextê°€ ìˆìœ¼ë©´ ì§ˆë¬¸ì— ì¶”ê°€
            enhanced_question = question
            if context:
                enhanced_question = f"{question}\n\nì¶”ê°€ ì¡°ê±´: {context}"
            
            # Azure OpenAI í˜¸ì¶œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•´ ì½œë°± ì‚¬ìš©
            try:
                from langchain_community.callbacks import get_openai_callback
            except ImportError:
                from langchain.callbacks import get_openai_callback
            
            with get_openai_callback() as cb:
                # Agent ì‹¤í–‰ (ë™ê¸° ë²„ì „)
                result = self.agent_executor.invoke({"input": enhanced_question})
                
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ì²˜ë¦¬
                output = result.get('output', '')
                
                # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ
                logger.info(f"ğŸ” ì½œë°± í† í° ì‚¬ìš©ëŸ‰: {cb.total_tokens} (prompt: {cb.prompt_tokens}, completion: {cb.completion_tokens})")
                
                # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ì€ ë™ê¸° ë²„ì „ì—ì„œ ìƒëµ (ë³µì¡ì„± ë°©ì§€)
                if user_id and cb.total_tokens > 0:
                    logger.info(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ìƒëµ (ë™ê¸° ë²„ì „) - ì‚¬ìš©ì: {user_id}, í† í°: {cb.total_tokens}")
            
            logger.info(
                f"âœ… LangChain Agent ë™ê¸° ì‹¤í–‰ ì™„ë£Œ - ì‹œê°„: {execution_time:.3f}s",
                extra={
                    'user_id': user_id,
                    'execution_time': execution_time,
                    'output_length': len(output),
                    'success': True
                }
            )
            
            response = {
                "success": True,
                "question": question,
                "answer": output,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions_sync",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation
            }
            
            if include_debug_info:
                response.update({
                    "debug_info": {
                        "intermediate_steps": result.get('intermediate_steps', []),
                        "tool_count": len(self.tools),
                        "max_iterations": self.agent_executor.max_iterations
                    }
                })
            
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            logger.error(
                f"âŒ LangChain Agent ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨ - ì‹œê°„: {execution_time:.3f}s",
                extra={
                    'user_id': user_id,
                    'execution_time': execution_time,
                    'error': error_msg,
                    'question': question[:100] + '...' if len(question) > 100 else question
                }
            )
            
            return {
                "success": False,
                "question": question,
                "error": error_msg,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions_sync",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation
            }

    async def execute_query_async(
        self,
        question: str,
        database: str = "northwind",
        context: Optional[str] = None,
        user_id: Optional[str] = None,
        include_explanation: bool = True,
        include_debug_info: bool = False,
        max_rows: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸° ë²„ì „ì˜ ì¿¼ë¦¬ ì‹¤í–‰.
        
        Args:
            question: ìì—°ì–´ ì§ˆë¬¸
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            include_debug_info: ë””ë²„ê·¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ¤– ë¹„ë™ê¸° LangChain Agent ì‹¤í–‰ ì‹œì‘ - ì§ˆë¬¸: {question[:50]}...")
            
            # Azure OpenAI í˜¸ì¶œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•´ ì½œë°± ì‚¬ìš©
            try:
                from langchain_community.callbacks import get_openai_callback
            except ImportError:
                from langchain.callbacks import get_openai_callback
            
            with get_openai_callback() as cb:
                # ë¹„ë™ê¸° ì‹¤í–‰
                result = await self.agent_executor.ainvoke({"input": question})
                
                # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ
                token_usage = {
                    "prompt_tokens": cb.prompt_tokens,
                    "completion_tokens": cb.completion_tokens,
                    "total_tokens": cb.total_tokens
                }
                
                # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
                token_service = self._get_token_usage_service()
                if user_id and token_service and cb.total_tokens > 0:
                    query_id = str(uuid.uuid4())
                    model_name = self.settings.azure_openai_deployment_name
                    
                    await token_service.record_token_usage(
                        user_id=user_id,
                        session_id="langchain_session",  # ì„¸ì…˜ ID ìƒì„± ë¡œì§ í•„ìš”
                        message_id=query_id,
                        token_usage=token_usage,
                        model_name=model_name,
                        query_type="text_to_sql",
                        additional_metadata={
                            "question": question,
                            "agent_type": "langchain_openai_functions",
                            "execution_time": time.time() - start_time
                        }
                    )
                    
                    logger.info(
                        f"ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì™„ë£Œ - ì´: {token_usage['total_tokens']}, "
                        f"ì…ë ¥: {token_usage['prompt_tokens']}, ì¶œë ¥: {token_usage['completion_tokens']}"
                    )
            
            execution_time = time.time() - start_time
            output = result.get('output', '')
            
            logger.info(f"âœ… ë¹„ë™ê¸° ì‹¤í–‰ ì™„ë£Œ - ì‹œê°„: {execution_time:.3f}s")
            
            response = {
                "success": True,
                "question": question,
                "answer": output,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions_async",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation,
                "token_usage": token_usage  # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ í¬í•¨
            }
            
            if include_debug_info:
                response["debug_info"] = {
                    "intermediate_steps": result.get('intermediate_steps', []),
                    "tool_count": len(self.tools)
                }
            
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            logger.error(f"âŒ ë¹„ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}")
            
            return {
                "success": False,
                "question": question,
                "error": error_msg,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions_async"
            }
    
    def batch_execute(
        self,
        questions: List[str],
        user_id: Optional[str] = None,
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            questions: ì§ˆë¬¸ ëª©ë¡
            user_id: ì‚¬ìš©ì ID
            max_concurrent: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜
            
        Returns:
            ê²°ê³¼ ëª©ë¡
        """
        start_time = time.time()
        results = []
        
        logger.info(f"ğŸ“‹ ë°°ì¹˜ ì‹¤í–‰ ì‹œì‘ - {len(questions)}ê°œ ì§ˆë¬¸")
        
        for i, question in enumerate(questions, 1):
            logger.info(f"ğŸ”„ ë°°ì¹˜ ì§„í–‰: {i}/{len(questions)} - {question[:30]}...")
            
            result = self.execute_query_sync(
                question=question,
                user_id=user_id,
                include_debug_info=False
            )
            
            # ë°°ì¹˜ ì •ë³´ ì¶”ê°€
            result["batch_info"] = {
                "batch_index": i,
                "total_questions": len(questions),
                "is_batch": True
            }
            
            results.append(result)
        
        total_time = time.time() - start_time
        successful_count = sum(1 for r in results if r.get('success', False))
        
        logger.info(
            f"âœ… ë°°ì¹˜ ì‹¤í–‰ ì™„ë£Œ - ì´ ì‹œê°„: {total_time:.3f}s, ì„±ê³µ: {successful_count}/{len(questions)}"
        )
        
        return results
    
    def get_agent_info(self) -> Dict[str, Any]:
        """
        Agent ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Agent ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "agent_type": "LangChain OpenAI Functions",
            "model": self.settings.azure_openai_deployment_name,
            "api_version": self.settings.azure_openai_api_version,
            "tools_count": len(self.tools),
            "tools": [tool.name for tool in self.tools],
            "simulation_mode": self.enable_simulation,
            "max_iterations": self.agent_executor.max_iterations,
            "database": "PostgreSQL Northwind",
            "supported_languages": ["Korean", "English"],
            "initialization_status": "ì™„ë£Œ"
        }
    
    def test_agent(self) -> Dict[str, Any]:
        """
        Agentì˜ ê¸°ë³¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        test_questions = [
            "ê³ ê° ìˆ˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
            "ì œí’ˆ ìˆ˜ëŠ” ëª‡ ê°œì¸ê°€ìš”?",
            "ì¹´í…Œê³ ë¦¬ë³„ ì œí’ˆ ìˆ˜ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
        ]
        
        logger.info("ğŸ§ª Agent ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_results = []
        start_time = time.time()
        
        for question in test_questions:
            result = self.execute_query_sync(question, user_id="test_user", include_debug_info=False)
            test_results.append({
                "question": question,
                "success": result.get("success", False),
                "execution_time": result.get("execution_time", 0),
                "has_answer": bool(result.get("answer", "").strip())
            })
        
        total_time = time.time() - start_time
        success_rate = sum(1 for r in test_results if r["success"]) / len(test_results) * 100
        
        test_summary = {
            "test_completed": True,
            "total_tests": len(test_questions),
            "success_rate": f"{success_rate:.1f}%",
            "total_time": f"{total_time:.3f}ì´ˆ",
            "avg_time": f"{total_time/len(test_questions):.3f}ì´ˆ",
            "results": test_results,
            "agent_info": self.get_agent_info()
        }
        
        logger.info(f"âœ… Agent í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µë¥ : {success_rate:.1f}%")
        
        return test_summary