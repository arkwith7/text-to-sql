"""
LangChain Agent for Text-to-SQL application.
Clean implementation with tiktoken-based token estimation.
"""

import logging
import time
import uuid
from typing import Dict, Any, Optional, List, TYPE_CHECKING

from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import AzureChatOpenAI

# TYPE_CHECKINGì„ ì‚¬ìš©í•˜ì—¬ ìˆœí™˜ import ë°©ì§€
if TYPE_CHECKING:
    from database.connection_manager import DatabaseManager

from core.tools.langchain_tools import setup_langchain_tools, get_langchain_tools
from core.config import get_settings
from core.utils.token_estimator import get_token_estimator
from utils.logging_config import setup_logging
from services.token_usage_service import TokenUsageService

logger = logging.getLogger(__name__)


class LangChainTextToSQLAgent:
    """
    LangChain ê¸°ë°˜ Text-to-SQL Agent.
    tiktokenì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ í† í° ì¶”ì • ê¸°ëŠ¥ í¬í•¨.
    """
    
    def __init__(
        self,
        db_manager: Optional["DatabaseManager"] = None,
        enable_simulation: bool = True,
        model_temperature: float = 0.1,
        max_iterations: int = 5,
        verbose: bool = True
    ):
        """
        LangChain Agent ì´ˆê¸°í™”.
        
        Args:
            db_manager: ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € (Optional, ìˆœí™˜ import ë°©ì§€)
            enable_simulation: ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ í™œì„±í™”
            model_temperature: LLM ì˜¨ë„ ì„¤ì •
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        """
        self.db_manager = db_manager
        self.enable_simulation = enable_simulation
        self.settings = get_settings()
        
        # Initialize token usage service (ì§€ì—° ë¡œë”©)
        self.token_usage_service = None
        
        # í† í° ì¶”ì •ê¸° ì´ˆê¸°í™”
        self.token_estimator = get_token_estimator()
        
        # ì„¤ì • ë¡œê¹…
        logger.info(f"LangChain Agent ì´ˆê¸°í™” ì‹œì‘")
        logger.info(f"  - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: {enable_simulation}")
        logger.info(f"  - ëª¨ë¸ ì˜¨ë„: {model_temperature}")
        logger.info(f"  - ìµœëŒ€ ë°˜ë³µ: {max_iterations}")
        logger.info(f"  - ìƒì„¸ ëª¨ë“œ: {verbose}")
        logger.info(f"  - í† í° ì¶”ì : tiktoken ê¸°ë°˜ ì¶”ì •")
        
        # Azure OpenAI LLM ì´ˆê¸°í™”
        self.llm = AzureChatOpenAI(
            azure_endpoint=self.settings.azure_openai_endpoint,
            api_key=self.settings.azure_openai_api_key,
            api_version=self.settings.azure_openai_api_version,
            azure_deployment=self.settings.azure_openai_deployment_name,
            temperature=model_temperature,
            max_tokens=2000,
            streaming=False,  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”
            model_kwargs={"seed": 42}  # ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ
        )
        logger.info("âœ… Azure OpenAI LLM ì´ˆê¸°í™” ì™„ë£Œ")
        
        # LangChain Tools ì„¤ì • (ì§€ì—° import ë°©ì§€ë¥¼ ìœ„í•´ Noneìœ¼ë¡œ ì „ë‹¬)
        setup_langchain_tools(None, enable_simulation)
        self.tools = get_langchain_tools()
        logger.info(f"âœ… LangChain Tools ì´ˆê¸°í™” ì™„ë£Œ - {len(self.tools)}ê°œ ë„êµ¬")
        
        # System prompt ì •ì˜
        self.system_prompt = """
ë‹¹ì‹ ì€ PostgreSQL Northwind ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ìì—°ì–´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ğŸ”’ ì¤‘ìš”í•œ ë³´ì•ˆ ê·œì¹™:
- ì´ ì‹œìŠ¤í…œì€ ì½ê¸° ì „ìš©(READ-ONLY)ì…ë‹ˆë‹¤
- SELECT ì¿¼ë¦¬ë§Œ í—ˆìš©ë©ë‹ˆë‹¤
- INSERT, UPDATE, DELETE, DROP, ALTER ë“±ì˜ ë°ì´í„° ë³€ê²½ ì‘ì—…ì€ ì ˆëŒ€ ê¸ˆì§€ë©ë‹ˆë‹¤
- ë°ì´í„°ë² ì´ìŠ¤ì˜ êµ¬ì¡°ë‚˜ ë°ì´í„°ë¥¼ ë³€ê²½í•˜ë ¤ëŠ” ì‹œë„ë¥¼ í•˜ì§€ ë§ˆì„¸ìš”

ì‘ì—… ìˆœì„œ:
1. get_database_schema ë„êµ¬ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ í™•ì¸
2. generate_sql_from_question ë„êµ¬ë¡œ SQL ì¿¼ë¦¬ ìƒì„±  
3. execute_sql_query_sync ë„êµ¬ë¡œ ì¿¼ë¦¬ ì‹¤í–‰
4. ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…
"""
        
        # Chat prompt í…œí”Œë¦¿ ìƒì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        # Agent ìƒì„± (ìµœì‹  API ì‚¬ìš©)
        self.agent = create_openai_functions_agent(
            self.llm, 
            self.tools, 
            prompt
        )
        
        # Agent Executor ìƒì„±
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=verbose,
            handle_parsing_errors=True,
            max_iterations=max_iterations,
            return_intermediate_steps=True  # ì¤‘ê°„ ë‹¨ê³„ ì •ë³´ ë°˜í™˜
        )
        
        logger.info("âœ… LangChain Agent ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_token_usage_service(self):
        """í† í° ì‚¬ìš©ëŸ‰ ì„œë¹„ìŠ¤ë¥¼ ì§€ì—° ë¡œë”©í•©ë‹ˆë‹¤."""
        if self.token_usage_service is None and self.db_manager:
            try:
                self.token_usage_service = TokenUsageService(self.db_manager)
                logger.info("âœ… Token Usage Service ì—°ë™ ì™„ë£Œ")
            except ImportError as e:
                logger.warning(f"Token Usage Service ë¡œë”© ì‹¤íŒ¨: {e}")
        return self.token_usage_service
    
    def _estimate_token_usage(
        self, 
        question: str, 
        answer: str,
        tool_calls: Optional[List[Dict]] = None
    ) -> Dict[str, int]:
        """í† í° ì‚¬ìš©ëŸ‰ì„ ì¶”ì •í•©ë‹ˆë‹¤."""
        try:
            return self.token_estimator.estimate_from_question_and_answer(
                question=question,
                answer=answer,
                system_prompt=self.system_prompt,
                tool_calls=tool_calls
            )
        except Exception as e:
            logger.warning(f"í† í° ì¶”ì • ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "prompt_tokens": max(100, len(question) // 4),
                "completion_tokens": max(50, len(answer) // 4),
                "total_tokens": max(150, (len(question) + len(answer)) // 4)
            }
    
    async def execute_query(
        self,
        question: str,
        database: str = "northwind",
        context: Optional[str] = None,
        user_id: Optional[str] = None,
        include_explanation: bool = True,
        include_debug_info: bool = False,
        max_rows: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ìì—°ì–´ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ì—¬ SQL ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            question: ìì—°ì–´ ì§ˆë¬¸
            database: ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ (ê¸°ë³¸ê°’: northwind)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            include_explanation: ì„¤ëª… í¬í•¨ ì—¬ë¶€
            include_debug_info: ë””ë²„ê·¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            max_rows: ìµœëŒ€ ë°˜í™˜ í–‰ ìˆ˜ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        session_id = str(uuid.uuid4())[:8]
        
        try:
            logger.info(
                f"ğŸš€ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ (ì„¸ì…˜: {session_id}) - "
                f"ì‚¬ìš©ì: {user_id or 'anonymous'}, "
                f"ì§ˆë¬¸: '{question[:100]}...'"
            )
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì§ˆë¬¸ì— ì¶”ê°€
            full_question = question
            if context:
                full_question = f"ì»¨í…ìŠ¤íŠ¸: {context}\n\nì§ˆë¬¸: {question}"
                logger.info(f"ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ë¨: {context[:100]}...")
            
            # Agent ì‹¤í–‰
            logger.info(f"ğŸ¤– LangChain Agent ì‹¤í–‰ ì¤‘...")
            result = await self.agent_executor.ainvoke({
                "input": full_question
            })
            
            # ê²°ê³¼ ì¶”ì¶œ
            answer = result.get("output", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            intermediate_steps = result.get("intermediate_steps", [])
            
            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = time.time() - start_time
            
            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •
            token_usage = self._estimate_token_usage(
                question=full_question,
                answer=answer,
                tool_calls=[step[0].tool for step in intermediate_steps if step and len(step) > 0]
            )
            
            logger.info(
                f"âœ… ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ (ì„¸ì…˜: {session_id}) - "
                f"ì‹¤í–‰ì‹œê°„: {execution_time:.3f}ì´ˆ, "
                f"í† í°: {token_usage['total_tokens']} "
                f"(ì…ë ¥: {token_usage['prompt_tokens']}, ì¶œë ¥: {token_usage['completion_tokens']})"
            )
            
            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ (ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥)
            if user_id and self._get_token_usage_service():
                try:
                    await self.token_usage_service.record_usage(
                        user_id=user_id,
                        session_id=session_id,
                        question=question,
                        model_name=self.settings.azure_openai_deployment_name,
                        prompt_tokens=token_usage["prompt_tokens"],
                        completion_tokens=token_usage["completion_tokens"],
                        total_tokens=token_usage["total_tokens"]
                    )
                    logger.info(f"ğŸ“Š í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì™„ë£Œ - ì‚¬ìš©ì: {user_id}")
                except Exception as e:
                    logger.warning(f"í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹¤íŒ¨: {e}")
            
            # ì¤‘ê°„ ë‹¨ê³„ì—ì„œ SQL ì¿¼ë¦¬ì™€ ê²°ê³¼ ì¶”ì¶œ
            sql_query = ""
            sql_results = []
            
            logger.info(f"ğŸ” ì¤‘ê°„ ë‹¨ê³„ ë¶„ì„ ì¤‘ - ì´ {len(intermediate_steps)}ê°œ ë‹¨ê³„")
            
            for i, step in enumerate(intermediate_steps):
                if step and len(step) >= 2:
                    action, observation = step[0], step[1]
                    
                    logger.info(f"ğŸ“‹ ë‹¨ê³„ {i+1}: action.tool = {getattr(action, 'tool', 'N/A')}")
                    logger.info(f"ğŸ“‹ ë‹¨ê³„ {i+1}: observation type = {type(observation)}")
                    logger.info(f"ğŸ“‹ ë‹¨ê³„ {i+1}: observation full content = {str(observation)}")
                    logger.info(f"ğŸ“‹ ë‹¨ê³„ {i+1}: observation preview = {str(observation)[:200]}...")
                    
                    # SQL ì‹¤í–‰ ë‹¨ê³„ì—ì„œ ì¿¼ë¦¬ì™€ ê²°ê³¼ ì¶”ì¶œ
                    if hasattr(action, 'tool') and action.tool == 'execute_sql_query_sync':
                        if hasattr(action, 'tool_input'):
                            sql_query = action.tool_input.get('sql_query', '')
                            logger.info(f"ğŸ” SQL ì¿¼ë¦¬ ì¶”ì¶œ: {sql_query[:100]}...")
                        
                        # observationì—ì„œ ê²°ê³¼ ì¶”ì¶œ
                        if isinstance(observation, str):
                            try:
                                # observationì´ JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                                import json
                                obs_data = json.loads(observation)
                                logger.info(f"ğŸ” íŒŒì‹±ëœ observation: {type(obs_data)}, keys: {list(obs_data.keys()) if isinstance(obs_data, dict) else 'N/A'}")
                                if isinstance(obs_data, dict):
                                    # SQL ì‹¤í–‰ ë„êµ¬ì˜ ì‘ë‹µ í˜•ì‹: {"success": True, "results": [...]}
                                    if 'results' in obs_data:
                                        sql_results = obs_data['results']
                                        logger.info(f"ğŸ” resultsì—ì„œ ë°ì´í„° ì¶”ì¶œ: {len(sql_results)}í–‰")
                                    elif 'data' in obs_data:
                                        sql_results = obs_data['data']
                                        logger.info(f"ğŸ” dataì—ì„œ ë°ì´í„° ì¶”ì¶œ: {len(sql_results)}í–‰")
                                elif isinstance(obs_data, list):
                                    sql_results = obs_data
                                    logger.info(f"ğŸ” ë¦¬ìŠ¤íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ: {len(sql_results)}í–‰")
                            except (json.JSONDecodeError, ValueError) as e:
                                # JSONì´ ì•„ë‹Œ ê²½ìš°, í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                                logger.warning(f"ğŸ” JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        elif isinstance(observation, (list, dict)):
                            if isinstance(observation, list):
                                sql_results = observation
                                logger.info(f"ğŸ” ì§ì ‘ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ: {len(sql_results)}í–‰")
                            elif isinstance(observation, dict):
                                # SQL ì‹¤í–‰ ë„êµ¬ì˜ ì‘ë‹µ í˜•ì‹: {"success": True, "results": [...]}
                                if 'results' in observation:
                                    sql_results = observation['results']
                                    logger.info(f"ğŸ” ì§ì ‘ resultsì—ì„œ ë°ì´í„° ì¶”ì¶œ: {len(sql_results)}í–‰")
                                elif 'data' in observation:
                                    sql_results = observation['data']
                                    logger.info(f"ğŸ” ì§ì ‘ dataì—ì„œ ë°ì´í„° ì¶”ì¶œ: {len(sql_results)}í–‰")
            
            logger.info(
                f"ğŸ” SQL ì •ë³´ ì¶”ì¶œ ì™„ë£Œ - "
                f"ì¿¼ë¦¬: {sql_query[:100]}{'...' if len(sql_query) > 100 else ''}, "
                f"ê²°ê³¼: {len(sql_results)}í–‰"
            )
            
            # ì‘ë‹µ êµ¬ì„±
            response = {
                "success": True,
                "answer": answer,
                "sql_query": sql_query,
                "results": sql_results,
                "question": question,
                "session_id": session_id,
                "execution_time": round(execution_time, 3),
                "token_usage": token_usage,
                "database": database,
                "timestamp": time.time()
            }
            
            # ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€ (ìš”ì²­ ì‹œ)
            if include_debug_info:
                response["debug_info"] = {
                    "agent_steps": len(intermediate_steps),
                    "tools_used": [step[0].tool for step in intermediate_steps if step and len(step) > 0],
                    "model_config": {
                        "deployment": self.settings.azure_openai_deployment_name,
                        "temperature": self.llm.temperature,
                        "max_tokens": self.llm.max_tokens
                    },
                    "system_prompt_length": len(self.system_prompt)
                }
            
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            
            logger.error(
                f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨ (ì„¸ì…˜: {session_id}) - "
                f"ì˜¤ë¥˜: {error_msg}, ì‹¤í–‰ì‹œê°„: {execution_time:.3f}ì´ˆ"
            )
            
            return {
                "success": False,
                "error": error_msg,
                "question": question,
                "session_id": session_id,
                "execution_time": round(execution_time, 3),
                "token_usage": None,
                "database": database,
                "timestamp": time.time()
            }
    
    def execute_query_sync(
        self,
        question: str,
        database: str = "northwind",
        context: Optional[str] = None,
        user_id: Optional[str] = None,
        include_explanation: bool = True,
        include_debug_info: bool = False,
        max_rows: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ë™ê¸°ì‹ ì§ˆë¬¸ ì²˜ë¦¬ (ë¹„ë™ê¸° ë˜í¼)
        """
        import asyncio
        
        try:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
            loop = asyncio.get_running_loop()
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    asyncio.run,
                    self.execute_query(
                        question, database, context, user_id,
                        include_explanation, include_debug_info, max_rows
                    )
                )
                return future.result()
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ì§ì ‘ ì‹¤í–‰
            return asyncio.run(
                self.execute_query(
                    question, database, context, user_id,
                    include_explanation, include_debug_info, max_rows
                )
            )
    
    def get_agent_info(self) -> Dict[str, Any]:
        """Agent ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "agent_type": "LangChain OpenAI Functions Agent",
            "model": self.settings.azure_openai_deployment_name,
            "temperature": self.llm.temperature,
            "max_tokens": self.llm.max_tokens,
            "tools": [tool.name for tool in self.tools],
            "tools_count": len(self.tools),  # ëˆ„ë½ëœ í‚¤ ì¶”ê°€
            "simulation_mode": self.enable_simulation,
            "max_iterations": self.agent_executor.max_iterations,
            "database": "PostgreSQL Northwind",
            "supported_languages": ["Korean", "English"],
            "token_estimation": "tiktoken ê¸°ë°˜",
            "initialization_status": "ì™„ë£Œ"
        }
    
    def test_agent(self) -> Dict[str, Any]:
        """
        Agentì˜ ê¸°ë³¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        test_questions = [
            "ê³ ê° ìˆ˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
            "ì œí’ˆ ìˆ˜ëŠ” ëª‡ ê°œì¸ê°€ìš”?",
            "ì¹´í…Œê³ ë¦¬ë³„ ì œí’ˆ ìˆ˜ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
        ]
        
        logger.info("ğŸ§ª Agent ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_results = []
        start_time = time.time()
        
        for question in test_questions:
            result = self.execute_query_sync(
                question, 
                user_id="test_user", 
                include_debug_info=False
            )
            test_results.append({
                "question": question,
                "success": result.get("success", False),
                "execution_time": result.get("execution_time", 0),
                "has_answer": bool(result.get("answer", "").strip()),
                "token_usage": result.get("token_usage")
            })
        
        total_time = time.time() - start_time
        success_rate = sum(1 for r in test_results if r["success"]) / len(test_results) * 100
        
        test_summary = {
            "test_completed": True,
            "total_tests": len(test_questions),
            "success_rate": f"{success_rate:.1f}%",
            "total_time": f"{total_time:.3f}ì´ˆ",
            "avg_time": f"{total_time/len(test_questions):.3f}ì´ˆ",
            "results": test_results,
            "agent_info": self.get_agent_info()
        }
        
        logger.info(f"âœ… Agent í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µë¥ : {success_rate:.1f}%")
        
        return test_summary
