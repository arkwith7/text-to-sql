"""
LangChain Agent for Text-to-SQL application.
Based on successful patterns from Jupyter notebook testing.
Uses latest LangChain APIs and patterns that have been proven to work.
Includes simple token usage estimation functionality.
"""

import logging
import time
import uuid
from typing import Dict, Any, Optional, List, TYPE_CHECKING

from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import AzureChatOpenAI
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from langchain_core.outputs import LLMResult

# TYPE_CHECKINGì„ ì‚¬ìš©í•˜ì—¬ ìˆœí™˜ import ë°©ì§€
if TYPE_CHECKING:
    from database.connection_manager import DatabaseManager

from core.tools.langchain_tools import setup_langchain_tools, get_langchain_tools
from core.config import get_settings
from core.utils.token_estimator import get_token_estimator
from utils.logging_config import setup_logging
from services.token_usage_service import TokenUsageService


class SimpleTokenCallbackHandler(BaseCallbackHandler):
    """ê°„ë‹¨í•œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬"""
    
    def __init__(self):
        super().__init__()
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.successful_requests = 0
        self.total_cost = 0.0
        
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLM ì‘ë‹µ ì™„ë£Œ ì‹œ ê¸°ë³¸ í† í° ì •ë³´ í™•ì¸"""
        try:
            logger = logging.getLogger(__name__)
            logger.debug(f"ğŸ” LLM ì‘ë‹µ ì™„ë£Œ - íƒ€ì…: {type(response)}")
            
            # í‘œì¤€ í† í° ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if response.llm_output and 'token_usage' in response.llm_output:
                token_usage = response.llm_output['token_usage']
                self.prompt_tokens += token_usage.get('prompt_tokens', 0)
                self.completion_tokens += token_usage.get('completion_tokens', 0)
                self.total_tokens += token_usage.get('total_tokens', 0)
                self.successful_requests += 1
                logger.info(f"ğŸ¯ í‘œì¤€ í† í° ì •ë³´ ì‚¬ìš©: {token_usage}")
                
        except Exception as e:
            logging.getLogger(__name__).debug(f"ì½œë°± í† í° ì¶”ì¶œ ì‹¤íŒ¨: {e}")

logger = logging.getLogger(__name__)
    """Azure OpenAI í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬"""
    
    def __init__(self):
        super().__init__()
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.successful_requests = 0
        self.total_cost = 0.0
        
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLM ì‘ë‹µ ì™„ë£Œ ì‹œ í† í° ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì‘ë‹µ êµ¬ì¡° ë””ë²„ê¹…
            logger = logging.getLogger(__name__)
            logger.info(f"ğŸ” LLM ì‘ë‹µ êµ¬ì¡°: {type(response)}")
            logger.info(f"ğŸ” LLM ì¶œë ¥: {response.llm_output}")
            
            # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ í† í° ì •ë³´ ì¶”ì¶œ ì‹œë„
            token_usage = None
            
            # ë°©ë²• 1: llm_outputì—ì„œ ì§ì ‘ ì¶”ì¶œ
            if response.llm_output and 'token_usage' in response.llm_output:
                token_usage = response.llm_output['token_usage']
                logger.info(f"ğŸ¯ ë°©ë²•1 - llm_outputì—ì„œ í† í° ì¶”ì¶œ: {token_usage}")
            
            # ë°©ë²• 2: generationsì˜ generation_infoì—ì„œ ì¶”ì¶œ
            if not token_usage and response.generations:
                logger.info(f"ğŸ” Generations ìˆ˜: {len(response.generations)}")
                for i, generation_list in enumerate(response.generations):
                    logger.info(f"ğŸ” Generation list {i}: {len(generation_list)} items")
                    for j, generation in enumerate(generation_list):
                        logger.info(f"ğŸ” Generation {i}-{j}: {type(generation)}")
                        logger.info(f"ğŸ” Generation attributes: {dir(generation)}")
                        if hasattr(generation, 'generation_info'):
                            gen_info = generation.generation_info
                            logger.info(f"ğŸ” Generation info: {gen_info}")
                            if gen_info and 'token_usage' in gen_info:
                                token_usage = gen_info['token_usage']
                                logger.info(f"ğŸ¯ ë°©ë²•2 - generation_infoì—ì„œ í† í° ì¶”ì¶œ: {token_usage}")
                                break
                        # response_metadataë„ í™•ì¸
                        if hasattr(generation, 'response_metadata'):
                            resp_meta = generation.response_metadata
                            logger.info(f"ğŸ” Response metadata: {resp_meta}")
                            if resp_meta and 'token_usage' in resp_meta:
                                token_usage = resp_meta['token_usage']
                                logger.info(f"ğŸ¯ ë°©ë²•2b - response_metadataì—ì„œ í† í° ì¶”ì¶œ: {token_usage}")
                                break
                    if token_usage:
                        break
            
            # ë°©ë²• 3: ì‘ë‹µì˜ ë‹¤ë¥¸ ì†ì„±ë“¤ í™•ì¸
            if not token_usage:
                logger.info(f"ğŸ” ì‘ë‹µ ì†ì„±ë“¤: {dir(response)}")
                for attr in ['usage', 'token_usage', 'usage_metadata']:
                    if hasattr(response, attr):
                        attr_value = getattr(response, attr)
                        logger.info(f"ğŸ” {attr}: {attr_value}")
                        if attr_value and isinstance(attr_value, dict):
                            token_usage = attr_value
                            break
            
            # í† í° ì •ë³´ê°€ ì¶”ì¶œë˜ì—ˆìœ¼ë©´ ì—…ë°ì´íŠ¸
            if token_usage and isinstance(token_usage, dict):
                self.prompt_tokens += token_usage.get('prompt_tokens', 0)
                self.completion_tokens += token_usage.get('completion_tokens', 0)
                self.total_tokens += token_usage.get('total_tokens', 0)
                self.successful_requests += 1
                
                # ê°„ë‹¨í•œ ë¹„ìš© ê³„ì‚° (GPT-4o-mini ê¸°ì¤€)
                input_cost = (self.prompt_tokens / 1000) * 0.00015  # $0.00015 per 1K input tokens
                output_cost = (self.completion_tokens / 1000) * 0.0006  # $0.0006 per 1K output tokens
                self.total_cost = input_cost + output_cost
                
                logger.info(
                    f"ğŸ¯ í† í° ì¶”ì¶œ ì„±ê³µ: {self.total_tokens} "
                    f"(prompt: {self.prompt_tokens}, completion: {self.completion_tokens})"
                )
            else:
                logger.warning(f"âŒ í† í° ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ - token_usage: {token_usage}")
                
        except Exception as e:
            logging.getLogger(__name__).warning(f"í† í° ì¶”ì¶œ ì‹¤íŒ¨: {e}")

logger = logging.getLogger(__name__)

class LangChainTextToSQLAgent:
    """
    LangChain ê¸°ë°˜ Text-to-SQL Agent.
    ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ ì„±ê³µí•œ íŒ¨í„´ì„ ì ìš©í•˜ì—¬ êµ¬í˜„ë¨.
    """
    
    def __init__(
        self,
        db_manager: Optional["DatabaseManager"] = None,
        enable_simulation: bool = True,
        model_temperature: float = 0.1,
        max_iterations: int = 5,
        verbose: bool = True
    ):
        """
        LangChain Agent ì´ˆê¸°í™”.
        
        Args:
            db_manager: ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € (Optional, ìˆœí™˜ import ë°©ì§€)
            enable_simulation: ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ í™œì„±í™”
            model_temperature: LLM ì˜¨ë„ ì„¤ì •
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        """
        self.db_manager = db_manager
        self.enable_simulation = enable_simulation
        self.settings = get_settings()
        
        # Initialize token usage service (ì§€ì—° ë¡œë”©)
        self.db_manager = db_manager
        self.token_usage_service = None
        
        # ì„¤ì • ë¡œê¹…
        logger.info(f"LangChain Agent ì´ˆê¸°í™” ì‹œì‘")
        logger.info(f"  - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: {enable_simulation}")
        logger.info(f"  - ëª¨ë¸ ì˜¨ë„: {model_temperature}")
        logger.info(f"  - ìµœëŒ€ ë°˜ë³µ: {max_iterations}")
        logger.info(f"  - ìƒì„¸ ëª¨ë“œ: {verbose}")
        logger.info(f"  - í† í° ì¶”ì : {'í™œì„±í™”' if db_manager else 'ë¹„í™œì„±í™”'}")
        
        # í† í° ì¶”ì ê¸° ë° íŒ¨ì²˜ë“¤ ì´ˆê¸°í™”
        token_tracker.reset()  # ì´ˆê¸°í™”
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ íŒ¨ì¹˜ (ë” í™•ì‹¤í•œ ë°©ë²•)
        self.openai_patcher = OpenAIClientPatcher(token_tracker)
        self.openai_patcher.patch_openai_client()
        logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ íŒ¨ì¹˜ ì™„ë£Œ")
        
        # HTTP ì¸í„°ì…‰í„°ë„ ì¶”ê°€ë¡œ ì‚¬ìš©
        self.http_interceptor = AzureOpenAIHTTPInterceptor(token_tracker)
        self.http_interceptor.patch_httpx_client()
        logger.info("âœ… HTTP ì¸í„°ì…‰í„° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Azure OpenAI LLM ì´ˆê¸°í™” (ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”ë¡œ usage ì •ë³´ í™•ë³´)
        self.llm = AzureChatOpenAI(
            azure_endpoint=self.settings.azure_openai_endpoint,
            api_key=self.settings.azure_openai_api_key,
            api_version=self.settings.azure_openai_api_version,
            azure_deployment=self.settings.azure_openai_deployment_name,
            temperature=model_temperature,
            max_tokens=2000,
            streaming=False  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”í•˜ì—¬ usage ì •ë³´ í¬í•¨
        )
        logger.info("âœ… Azure OpenAI LLM ì´ˆê¸°í™” ì™„ë£Œ")
        
        # LangChain Tools ì„¤ì • (ì§€ì—° import ë°©ì§€ë¥¼ ìœ„í•´ Noneìœ¼ë¡œ ì „ë‹¬)
        setup_langchain_tools(None, enable_simulation)
        self.tools = get_langchain_tools()
        logger.info(f"âœ… LangChain Tools ì´ˆê¸°í™” ì™„ë£Œ - {len(self.tools)}ê°œ ë„êµ¬")
        
        # System prompt ì •ì˜ (ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ ì„±ê³µí•œ íŒ¨í„´ + ë³´ì•ˆ ê°•í™”)
        self.system_prompt = """
ë‹¹ì‹ ì€ PostgreSQL Northwind ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ìì—°ì–´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ğŸ”’ ì¤‘ìš”í•œ ë³´ì•ˆ ê·œì¹™:
- ì´ ì‹œìŠ¤í…œì€ ì½ê¸° ì „ìš©(READ-ONLY)ì…ë‹ˆë‹¤
- SELECT ì¿¼ë¦¬ë§Œ í—ˆìš©ë©ë‹ˆë‹¤
- INSERT, UPDATE, DELETE, DROP, ALTER ë“±ì˜ ë°ì´í„° ë³€ê²½ ì‘ì—…ì€ ì ˆëŒ€ ê¸ˆì§€ë©ë‹ˆë‹¤
- ë°ì´í„°ë² ì´ìŠ¤ì˜ êµ¬ì¡°ë‚˜ ë°ì´í„°ë¥¼ ë³€ê²½í•˜ë ¤ëŠ” ì‹œë„ë¥¼ í•˜ì§€ ë§ˆì„¸ìš”

ì‘ì—… ìˆœì„œ:
1. get_database_schema ë„êµ¬ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ í™•ì¸
2. generate_sql_from_question ë„êµ¬ë¡œ SQL ì¿¼ë¦¬ ìƒì„±  
3. execute_sql_query_sync ë„êµ¬ë¡œ ì¿¼ë¦¬ ì‹¤í–‰
4. ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…
"""
        
        # Chat prompt í…œí”Œë¦¿ ìƒì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        # Agent ìƒì„± (ìµœì‹  API ì‚¬ìš©)
        self.agent = create_openai_functions_agent(
            self.llm, 
            self.tools, 
            prompt
        )
        
        # Agent Executor ìƒì„±
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=verbose,
            handle_parsing_errors=True,
            max_iterations=max_iterations,
            return_intermediate_steps=True  # ì¤‘ê°„ ë‹¨ê³„ ì •ë³´ ë°˜í™˜
        )
        
        logger.info("âœ… LangChain Agent ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_token_usage_service(self):
        """í† í° ì‚¬ìš©ëŸ‰ ì„œë¹„ìŠ¤ë¥¼ ì§€ì—° ë¡œë”©í•©ë‹ˆë‹¤."""
        if self.token_usage_service is None and self.db_manager:
            try:
                self.token_usage_service = TokenUsageService(self.db_manager)
                logger.info("âœ… LangChain Token Usage Service ì—°ë™ ì™„ë£Œ")
            except ImportError as e:
                logger.warning(f"Token Usage Service ë¡œë”© ì‹¤íŒ¨: {e}")
        return self.token_usage_service
    
    async def execute_query(
        self,
        question: str,
        database: str = "northwind",
        context: Optional[str] = None,
        user_id: Optional[str] = None,
        include_explanation: bool = True,
        include_debug_info: bool = False,
        max_rows: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ìì—°ì–´ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ì—¬ SQL ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            question: ìì—°ì–´ ì§ˆë¬¸
            database: ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ (ê¸°ë³¸ê°’: northwind)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            include_explanation: ì„¤ëª… í¬í•¨ ì—¬ë¶€
            include_debug_info: ë””ë²„ê·¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            max_rows: ìµœëŒ€ ë°˜í™˜ í–‰ ìˆ˜ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            logger.info(
                f"ğŸ¤– LangChain Agent ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘",
                extra={
                    'user_id': user_id,
                    'question': question[:100] + '...' if len(question) > 100 else question,
                    'database': database,
                    'include_debug': include_debug_info
                }
            )
            
            # Contextê°€ ìˆìœ¼ë©´ ì§ˆë¬¸ì— ì¶”ê°€
            enhanced_question = question
            if context:
                enhanced_question = f"{question}\n\nì¶”ê°€ ì¡°ê±´: {context}"
            
            # Azure OpenAI í˜¸ì¶œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•´ ì½œë°± ì‚¬ìš©
            try:
                from langchain_community.callbacks import get_openai_callback
                
                # í‘œì¤€ OpenAI ì½œë°±ê³¼ ì»¤ìŠ¤í…€ ì½œë°±ì„ í•¨ê»˜ ì‚¬ìš©
                token_callback = AzureOpenAITokenCallbackHandler()
                
                logger.info("âœ… LangChain Community ì½œë°± ë¡œë“œ ì„±ê³µ")
                
                # HTTP ì¸í„°ì…‰í„° í† í° ì¶”ì ê¸° ì´ˆê¸°í™”
                token_tracker.reset()
                
                # get_openai_callbackìœ¼ë¡œ ë˜í•‘í•˜ì—¬ ì‹¤í–‰
                with get_openai_callback() as cb:
                    logger.info("ğŸ” í† í° ì½œë°± ì‹œì‘ - Agent ì‹¤í–‰ ì „")
                    logger.info("ğŸ¤– Agent executor ainvoke ì‹œì‘")
                    
                    # ë‘ ê°œì˜ ì½œë°±ì„ í•¨ê»˜ ì‚¬ìš©
                    result = await self.agent_executor.ainvoke(
                        {"input": enhanced_question}, 
                        {"callbacks": [token_callback]}
                    )
                    execution_time = time.time() - start_time
                    
                    logger.info("ğŸ¤– Agent executor ainvoke ì™„ë£Œ - ê²°ê³¼ í‚¤: %s", list(result.keys()))
                    
                    # HTTP ì¸í„°ì…‰í„°ì—ì„œ í† í° ì •ë³´ ì¶”ì¶œ
                    http_tokens = token_tracker.get_token_usage()
                    logger.info(f"ğŸ“Š HTTP ì¸í„°ì…‰í„° í† í° ì •ë³´: {http_tokens}")
                    
                    # í‘œì¤€ ì½œë°±ì—ì„œ í† í° ì •ë³´ ì¶”ì¶œ
                    standard_tokens = {
                        "total_tokens": cb.total_tokens,
                        "prompt_tokens": cb.prompt_tokens,
                        "completion_tokens": cb.completion_tokens,
                        "total_cost": cb.total_cost,
                        "successful_requests": cb.successful_requests
                    }
                    
                    logger.info(f"ğŸ“Š í‘œì¤€ ì½œë°± í† í° ì •ë³´: {standard_tokens}")
                    
                    # ì»¤ìŠ¤í…€ ì½œë°±ì—ì„œë„ ì •ë³´ ì¶”ì¶œ
                    custom_tokens = {
                        "total_tokens": token_callback.total_tokens,
                        "prompt_tokens": token_callback.prompt_tokens,
                        "completion_tokens": token_callback.completion_tokens,
                        "total_cost": token_callback.total_cost,
                        "successful_requests": token_callback.successful_requests
                    }
                    
                    logger.info(f"ğŸ“Š ì»¤ìŠ¤í…€ ì½œë°± í† í° ì •ë³´: {custom_tokens}")
                    
                    # í† í° ì •ë³´ ìš°ì„ ìˆœìœ„: HTTP ì¸í„°ì…‰í„° > í‘œì¤€ ì½œë°± > ì»¤ìŠ¤í…€ ì½œë°±
                    if http_tokens["total_tokens"] > 0:
                        final_token_usage = http_tokens
                        logger.info("ğŸ¯ HTTP ì¸í„°ì…‰í„° í† í° ì •ë³´ ì‚¬ìš©")
                    elif standard_tokens["total_tokens"] > 0:
                        final_token_usage = standard_tokens
                        logger.info("ğŸ¯ í‘œì¤€ ì½œë°± í† í° ì •ë³´ ì‚¬ìš©")
                    else:
                        final_token_usage = custom_tokens
                        logger.info("ğŸ¯ ì»¤ìŠ¤í…€ ì½œë°± í† í° ì •ë³´ ì‚¬ìš© (fallback)")
                    
            except ImportError:
                logger.warning("âš ï¸ LangChain Community ì½œë°±ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ê¸°ë³¸ ì‹¤í–‰")
                # ì½œë°± ì—†ì´ ì‹¤í–‰
                token_callback = AzureOpenAITokenCallbackHandler()
                
                # HTTP ì¸í„°ì…‰í„° í† í° ì¶”ì ê¸° ì´ˆê¸°í™”
                token_tracker.reset()
                
                result = await self.agent_executor.ainvoke(
                    {"input": enhanced_question}, 
                    {"callbacks": [token_callback]}
                )
                execution_time = time.time() - start_time
                
                # HTTP ì¸í„°ì…‰í„°ì—ì„œ í† í° ì •ë³´ ì‹œë„
                http_tokens = token_tracker.get_token_usage()
                if http_tokens["total_tokens"] > 0:
                    final_token_usage = http_tokens
                    logger.info("ğŸ¯ HTTP ì¸í„°ì…‰í„° í† í° ì •ë³´ ì‚¬ìš© (fallback)")
                else:
                    # í† í° ì •ë³´ëŠ” 0ìœ¼ë¡œ ì„¤ì •
                    final_token_usage = {
                        "total_tokens": 0,
                        "prompt_tokens": 0, 
                        "completion_tokens": 0,
                        "total_cost": 0.0,
                        "successful_requests": 0
                    }
                    logger.warning("âŒ ëª¨ë“  í† í° ì¶”ì  ë°©ë²• ì‹¤íŒ¨")            
            # ê³µí†µ ê²°ê³¼ ì²˜ë¦¬
            output = result.get('output', '')
            intermediate_steps = result.get('intermediate_steps', [])
            
            logger.info(f"ğŸ” Agent ì¶œë ¥ ê¸¸ì´: {len(output)} ë¬¸ì")
            logger.info(f"'{output}'")
            logger.info(f"ğŸ” ì¤‘ê°„ ë‹¨ê³„ ìˆ˜: {len(intermediate_steps)}")
            
            # Check if agent failed due to iteration/time limit and provide user-friendly message
            is_agent_limit_error = (
                "Agent stopped due to iteration limit or time limit" in output or
                output.strip() == "Agent stopped due to iteration limit or time limit."
            )
            
            if is_agent_limit_error:
                logger.warning("âš ï¸ Agentê°€ iteration/time limitìœ¼ë¡œ ì¸í•´ ì¤‘ë‹¨ë¨")
                output = "ì§ˆë¬¸ì´ ë„ˆë¬´ ë³µì¡í•˜ê±°ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°ì— ë§ëŠ” ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”."
            
            # ì¤‘ê°„ ë‹¨ê³„ì—ì„œ SQL ì¿¼ë¦¬ì™€ ê²°ê³¼ ì¶”ì¶œ
            executed_sql = None
            sql_results = []
            
            for i, (agent_action, observation) in enumerate(intermediate_steps, 1):
                observation_str = str(observation)
                logger.info(f"ğŸ” ë‹¨ê³„ {i}: ë„êµ¬={agent_action.tool}, ê´€ì°°={observation_str[:200]}...")
                
                # execute_sql_query_sync ë„êµ¬ì˜ ê²°ê³¼ì—ì„œ SQLê³¼ ë°ì´í„° ì¶”ì¶œ
                if agent_action.tool == "execute_sql_query_sync":
                    try:
                        # ê´€ì°°ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                        if isinstance(observation, dict):
                            obs_dict = observation
                        elif isinstance(observation, str):
                            # ë¬¸ìì—´ì´ë©´ JSON íŒŒì‹± ì‹œë„
                            import json
                            try:
                                obs_dict = json.loads(observation)
                            except json.JSONDecodeError:
                                logger.warning(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {observation[:100]}...")
                                continue
                        else:
                            logger.warning(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ê´€ì°° íƒ€ì…: {type(observation)}")
                            continue
                        
                        # SQL ì¿¼ë¦¬ì™€ ê²°ê³¼ ì¶”ì¶œ
                        if obs_dict.get('success'):
                            if obs_dict.get('sql_query'):
                                executed_sql = obs_dict.get('sql_query')
                                sql_results = obs_dict.get('results', [])
                                logger.info(f"ğŸ¯ SQL ì¿¼ë¦¬ ì¶”ì¶œ ì„±ê³µ: {executed_sql[:100]}...")
                                logger.info(f"ğŸ¯ ê²°ê³¼ í–‰ ìˆ˜: {len(sql_results)}")
                                break  # ì²« ë²ˆì§¸ ì„±ê³µí•œ SQL ì‹¤í–‰ ê²°ê³¼ ì‚¬ìš©
                            else:
                                logger.warning(f"âš ï¸ SQL ì¿¼ë¦¬ê°€ ì—†ìŒ: {obs_dict}")
                        else:
                            logger.warning(f"âš ï¸ SQL ì‹¤í–‰ ì‹¤íŒ¨: {obs_dict}")
                    except Exception as e:
                        logger.error(f"âŒ SQL ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}, ê´€ì°°: {observation_str[:100]}...")
            
            # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ ë° ê¸°ë¡
            logger.info(f"ğŸ” ìµœì¢… í† í° ì‚¬ìš©ëŸ‰: {final_token_usage['total_tokens']} (prompt: {final_token_usage['prompt_tokens']}, completion: {final_token_usage['completion_tokens']})")
            logger.info(f"ğŸ” ì‚¬ìš©ì ID: {user_id}")
            
            # í† í° ì‚¬ìš©ëŸ‰ì´ ìˆìœ¼ë©´ ê¸°ë¡
            if final_token_usage['total_tokens'] > 0 and user_id:
                try:
                    token_service = self._get_token_usage_service()
                    if token_service:
                        await token_service.record_token_usage(
                            user_id=user_id,
                            model=self.settings.azure_openai_deployment_name,
                            prompt_tokens=final_token_usage['prompt_tokens'],
                            completion_tokens=final_token_usage['completion_tokens'],
                            total_tokens=final_token_usage['total_tokens'],
                            cost_estimate=final_token_usage['total_cost']
                        )
                        logger.info("âœ… í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì™„ë£Œ")
                    else:
                        logger.warning("í† í° ì‚¬ìš©ëŸ‰ ì„œë¹„ìŠ¤ ì—†ìŒ")
                except Exception as e:
                    logger.error(f"âŒ í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹¤íŒ¨: {e}")
            else:
                logger.warning(f"í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì¡°ê±´ ë¯¸ì¶©ì¡± - user_id: {user_id}, tokens: {final_token_usage['total_tokens']}")
            
            logger.info(
                f"âœ… LangChain Agent ì‹¤í–‰ ì™„ë£Œ - ì‹œê°„: {execution_time:.3f}s",
                extra={
                    'user_id': user_id,
                    'execution_time': execution_time,
                    'output_length': len(output),
                    'success': True
                }
            )
            
            # ìµœì¢… ì‘ë‹µ ìƒì„±
            response = {
                "success": True,
                "question": question,
                "answer": output,
                "sql_query": executed_sql or "",
                "results": sql_results or [],
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation,
                "explanation": output,  # Agentì˜ ë‹µë³€ì„ ì„¤ëª…ìœ¼ë¡œë„ ì‚¬ìš©
                "token_usage": final_token_usage
            }
            
            if include_debug_info:
                response.update({
                    "debug_info": {
                        "intermediate_steps": intermediate_steps,
                        "tool_count": len(self.tools),
                        "max_iterations": self.agent_executor.max_iterations,
                        "executed_sql": executed_sql,
                        "sql_results_count": len(sql_results) if sql_results else 0
                    }
                })
            
            logger.info(f"âœ… LangChain Agent ì‹¤í–‰ ì™„ë£Œ - ì‹œê°„: {execution_time:.3f}s")
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"LangChain Agent ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
            logger.error(error_msg, exc_info=True)
            
            return {
                "success": False,
                "question": question,
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sql_query": "",
                "results": [],
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation,
                "explanation": error_msg,
                "token_usage": {
                    "total_tokens": 0,
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_cost": 0.0,
                    "successful_requests": 0
                }
            }
    
    def execute_query_sync(
        self,
        question: str,
        database: str = "northwind",
        context: Optional[str] = None,
        user_id: Optional[str] = None,
        include_explanation: bool = True,
        include_debug_info: bool = False,
        max_rows: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ë™ê¸° ë²„ì „ì˜ ì¿¼ë¦¬ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬ìš©).
        
        Args:
            question: ìì—°ì–´ ì§ˆë¬¸
            database: ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ (ê¸°ë³¸ê°’: northwind)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            include_explanation: ì„¤ëª… í¬í•¨ ì—¬ë¶€
            include_debug_info: ë””ë²„ê·¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            max_rows: ìµœëŒ€ ë°˜í™˜ í–‰ ìˆ˜ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            logger.info(
                f"ğŸ¤– LangChain Agent ë™ê¸° ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘",
                extra={
                    'user_id': user_id,
                    'question': question[:100] + '...' if len(question) > 100 else question,
                    'database': database,
                    'include_debug': include_debug_info
                }
            )
            
            # Contextê°€ ìˆìœ¼ë©´ ì§ˆë¬¸ì— ì¶”ê°€
            enhanced_question = question
            if context:
                enhanced_question = f"{question}\n\nì¶”ê°€ ì¡°ê±´: {context}"
            
            # Azure OpenAI í˜¸ì¶œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•´ ì½œë°± ì‚¬ìš©
            try:
                from langchain_community.callbacks import get_openai_callback
            except ImportError:
                from langchain.callbacks import get_openai_callback
            
            with get_openai_callback() as token_callback:
                # Agent ì‹¤í–‰ (ë™ê¸° ë²„ì „)
                result = self.agent_executor.invoke({"input": enhanced_question})
                
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ì²˜ë¦¬
                output = result.get('output', '')
                
                # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ
                logger.info(f"ğŸ” ì½œë°± í† í° ì‚¬ìš©ëŸ‰: {token_callback.total_tokens} (prompt: {token_callback.prompt_tokens}, completion: {token_callback.completion_tokens})")
                
                # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ì€ ë™ê¸° ë²„ì „ì—ì„œ ìƒëµ (ë³µì¡ì„± ë°©ì§€)
                if user_id and token_callback.total_tokens > 0:
                    logger.info(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ìƒëµ (ë™ê¸° ë²„ì „) - ì‚¬ìš©ì: {user_id}, í† í°: {token_callback.total_tokens}")
            
            logger.info(
                f"âœ… LangChain Agent ë™ê¸° ì‹¤í–‰ ì™„ë£Œ - ì‹œê°„: {execution_time:.3f}s",
                extra={
                    'user_id': user_id,
                    'execution_time': execution_time,
                    'output_length': len(output),
                    'success': True
                }
            )
            
            # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ê°€
            token_info = {
                "prompt_tokens": token_callback.prompt_tokens if token_callback else 0,
                "completion_tokens": token_callback.completion_tokens if token_callback else 0,
                "total_tokens": token_callback.total_tokens if token_callback else 0
            }
            
            response = {
                "success": True,
                "question": question,
                "answer": output,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions_sync",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation,
                "token_usage": token_info  # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ê°€
            }
            
            if include_debug_info:
                response.update({
                    "debug_info": {
                        "intermediate_steps": result.get('intermediate_steps', []),
                        "tool_count": len(self.tools),
                        "max_iterations": self.agent_executor.max_iterations
                    }
                })
            
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            logger.error(
                f"âŒ LangChain Agent ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨ - ì‹œê°„: {execution_time:.3f}s",
                extra={
                    'user_id': user_id,
                    'execution_time': execution_time,
                    'error': error_msg,
                    'question': question[:100] + '...' if len(question) > 100 else question
                }
            )
            
            return {
                "success": False,
                "question": question,
                "error": error_msg,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions_sync",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation
            }

    async def execute_query_async(
        self,
        question: str,
        database: str = "northwind",
        context: Optional[str] = None,
        user_id: Optional[str] = None,
        include_explanation: bool = True,
        include_debug_info: bool = False,
        max_rows: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸° ë²„ì „ì˜ ì¿¼ë¦¬ ì‹¤í–‰.
        
        Args:
            question: ìì—°ì–´ ì§ˆë¬¸
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            include_debug_info: ë””ë²„ê·¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ¤– ë¹„ë™ê¸° LangChain Agent ì‹¤í–‰ ì‹œì‘ - ì§ˆë¬¸: {question[:50]}...")
            
            # Azure OpenAI í˜¸ì¶œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•´ ì½œë°± ì‚¬ìš©
            try:
                from langchain_community.callbacks import get_openai_callback
            except ImportError:
                from langchain.callbacks import get_openai_callback
            
            with get_openai_callback() as token_callback:
                # ë¹„ë™ê¸° ì‹¤í–‰
                result = await self.agent_executor.ainvoke({"input": question})
                
                # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ
                token_usage = {
                    "prompt_tokens": token_callback.prompt_tokens,
                    "completion_tokens": token_callback.completion_tokens,
                    "total_tokens": token_callback.total_tokens
                }
                
                # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
                token_service = self._get_token_usage_service()
                if user_id and token_service and token_callback.total_tokens > 0:
                    query_id = str(uuid.uuid4())
                    model_name = self.settings.azure_openai_deployment_name
                    
                    await token_service.record_token_usage(
                        user_id=user_id,
                        session_id="langchain_session",  # ì„¸ì…˜ ID ìƒì„± ë¡œì§ í•„ìš”
                        message_id=query_id,
                        token_usage=token_usage,
                        model_name=model_name,
                        query_type="text_to_sql",
                        additional_metadata={
                            "question": question,
                            "agent_type": "langchain_openai_functions",
                            "execution_time": time.time() - start_time
                        }
                    )
                    
                    logger.info(
                        f"ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì™„ë£Œ - ì´: {token_usage['total_tokens']}, "
                        f"ì…ë ¥: {token_usage['prompt_tokens']}, ì¶œë ¥: {token_usage['completion_tokens']}"
                    )
            
            execution_time = time.time() - start_time
            output = result.get('output', '')
            
            logger.info(f"âœ… ë¹„ë™ê¸° ì‹¤í–‰ ì™„ë£Œ - ì‹œê°„: {execution_time:.3f}s")
            
            response = {
                "success": True,
                "question": question,
                "answer": output,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions_async",
                "model": self.settings.azure_openai_deployment_name,
                "simulation_mode": self.enable_simulation,
                "token_usage": token_usage  # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ í¬í•¨
            }
            
            if include_debug_info:
                response["debug_info"] = {
                    "intermediate_steps": result.get('intermediate_steps', []),
                    "tool_count": len(self.tools)
                }
            
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            logger.error(f"âŒ ë¹„ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}")
            
            return {
                "success": False,
                "question": question,
                "error": error_msg,
                "execution_time": execution_time,
                "agent_type": "langchain_openai_functions_async"
            }
    
    def batch_execute(
        self,
        questions: List[str],
        user_id: Optional[str] = None,
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            questions: ì§ˆë¬¸ ëª©ë¡
            user_id: ì‚¬ìš©ì ID
            max_concurrent: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜
            
        Returns:
            ê²°ê³¼ ëª©ë¡
        """
        start_time = time.time()
        results = []
        
        logger.info(f"ğŸ“‹ ë°°ì¹˜ ì‹¤í–‰ ì‹œì‘ - {len(questions)}ê°œ ì§ˆë¬¸")
        
        for i, question in enumerate(questions, 1):
            logger.info(f"ğŸ”„ ë°°ì¹˜ ì§„í–‰: {i}/{len(questions)} - {question[:30]}...")
            
            result = self.execute_query_sync(
                question=question,
                user_id=user_id,
                include_debug_info=False
            )
            
            # ë°°ì¹˜ ì •ë³´ ì¶”ê°€
            result["batch_info"] = {
                "batch_index": i,
                "total_questions": len(questions),
                "is_batch": True
            }
            
            results.append(result)
        
        total_time = time.time() - start_time
        successful_count = sum(1 for r in results if r.get('success', False))
        
        logger.info(
            f"âœ… ë°°ì¹˜ ì‹¤í–‰ ì™„ë£Œ - ì´ ì‹œê°„: {total_time:.3f}s, ì„±ê³µ: {successful_count}/{len(questions)}"
        )
        
        return results
    
    def get_agent_info(self) -> Dict[str, Any]:
        """
        Agent ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Agent ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "agent_type": "LangChain OpenAI Functions",
            "model": self.settings.azure_openai_deployment_name,
            "api_version": self.settings.azure_openai_api_version,
            "tools_count": len(self.tools),
            "tools": [tool.name for tool in self.tools],
            "simulation_mode": self.enable_simulation,
            "max_iterations": self.agent_executor.max_iterations,
            "database": "PostgreSQL Northwind",
            "supported_languages": ["Korean", "English"],
            "initialization_status": "ì™„ë£Œ"
        }
    
    def test_agent(self) -> Dict[str, Any]:
        """
        Agentì˜ ê¸°ë³¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        test_questions = [
            "ê³ ê° ìˆ˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
            "ì œí’ˆ ìˆ˜ëŠ” ëª‡ ê°œì¸ê°€ìš”?",
            "ì¹´í…Œê³ ë¦¬ë³„ ì œí’ˆ ìˆ˜ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
        ]
        
        logger.info("ğŸ§ª Agent ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_results = []
        start_time = time.time()
        
        for question in test_questions:
            result = self.execute_query_sync(question, user_id="test_user", include_debug_info=False)
            test_results.append({
                "question": question,
                "success": result.get("success", False),
                "execution_time": result.get("execution_time", 0),
                "has_answer": bool(result.get("answer", "").strip())
            })
        
        total_time = time.time() - start_time
        success_rate = sum(1 for r in test_results if r["success"]) / len(test_results) * 100
        
        test_summary = {
            "test_completed": True,
            "total_tests": len(test_questions),
            "success_rate": f"{success_rate:.1f}%",
            "total_time": f"{total_time:.3f}ì´ˆ",
            "avg_time": f"{total_time/len(test_questions):.3f}ì´ˆ",
            "results": test_results,
            "agent_info": self.get_agent_info()
        }
        
        logger.info(f"âœ… Agent í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µë¥ : {success_rate:.1f}%")
        
        return test_summary