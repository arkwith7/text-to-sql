#!/usr/bin/env python3
"""
ë¡œê·¸ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸
Text-to-SQL AI Agentì˜ ë¡œê·¸ë¥¼ ê´€ë¦¬í•˜ê³  ë¶„ì„í•˜ëŠ” í¸ì˜ ë„êµ¬
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils.log_analyzer import LogAnalyzer
import argparse
import json
from datetime import datetime, timedelta
from pathlib import Path

def show_log_status():
    """ë¡œê·¸ íŒŒì¼ ìƒíƒœ í‘œì‹œ"""
    print("ğŸ“Š ë¡œê·¸ íŒŒì¼ ìƒíƒœ:")
    print("-" * 50)
    
    log_dir = Path("logs")
    if not log_dir.exists():
        print("âš ï¸ ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    log_files = {
        'app.log': 'ì „ì²´ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸',
        'error.log': 'ì—ëŸ¬ ë¡œê·¸',
        'api_requests.log': 'API ìš”ì²­ ë¡œê·¸',
        'sql_queries.log': 'SQL ì¿¼ë¦¬ ë¡œê·¸',
        'chat_sessions.log': 'ì±„íŒ… ì„¸ì…˜ ë¡œê·¸',
        'authentication.log': 'ì¸ì¦ ë¡œê·¸'
    }
    
    for filename, description in log_files.items():
        filepath = log_dir / filename
        if filepath.exists():
            stat = filepath.stat()
            size_mb = stat.st_size / (1024 * 1024)
            modified = datetime.fromtimestamp(stat.st_mtime)
            print(f"âœ… {filename:<20} | {size_mb:>8.2f}MB | ìˆ˜ì •: {modified.strftime('%Y-%m-%d %H:%M:%S')} | {description}")
        else:
            print(f"âŒ {filename:<20} | íŒŒì¼ ì—†ìŒ      |                    | {description}")

def analyze_logs(hours=24, detailed=False):
    """ë¡œê·¸ ë¶„ì„ ì‹¤í–‰"""
    print(f"ğŸ” ìµœê·¼ {hours}ì‹œê°„ ë¡œê·¸ ë¶„ì„ ì¤‘...")
    print("-" * 50)
    
    analyzer = LogAnalyzer()
    
    # ì—ëŸ¬ ë¶„ì„
    print("ğŸ“ˆ ì—ëŸ¬ ë¶„ì„:")
    error_analysis = analyzer.analyze_errors(hours)
    print(f"  - ì´ ì—ëŸ¬: {error_analysis['total_errors']}ê°œ")
    if error_analysis['error_by_module']:
        print("  - ëª¨ë“ˆë³„ ì—ëŸ¬:")
        for module, count in error_analysis['error_by_module'].items():
            print(f"    * {module}: {count}ê°œ")
    print()
    
    # API ì„±ëŠ¥ ë¶„ì„
    print("ğŸš€ API ì„±ëŠ¥ ë¶„ì„:")
    api_analysis = analyzer.analyze_api_performance(hours)
    print(f"  - ì´ ìš”ì²­: {api_analysis['total_requests']}ê°œ")
    print(f"  - í‰ê·  ì‘ë‹µì‹œê°„: {api_analysis['avg_response_time_ms']:.1f}ms")
    print(f"  - ìµœëŒ€ ì‘ë‹µì‹œê°„: {api_analysis['max_response_time_ms']:.1f}ms")
    if api_analysis['slow_requests']:
        print(f"  - ëŠë¦° ìš”ì²­: {len(api_analysis['slow_requests'])}ê°œ")
    print()
    
    # SQL ì¿¼ë¦¬ ë¶„ì„
    print("ğŸ—„ï¸ SQL ì¿¼ë¦¬ ë¶„ì„:")
    sql_analysis = analyzer.analyze_sql_queries(hours)
    print(f"  - ì´ ì¿¼ë¦¬: {sql_analysis['total_queries']}ê°œ")
    print(f"  - í‰ê·  ì‹¤í–‰ì‹œê°„: {sql_analysis['avg_execution_time']:.3f}ì´ˆ")
    if sql_analysis['slow_queries']:
        print(f"  - ëŠë¦° ì¿¼ë¦¬: {len(sql_analysis['slow_queries'])}ê°œ")
    if sql_analysis['failed_queries']:
        print(f"  - ì‹¤íŒ¨í•œ ì¿¼ë¦¬: {len(sql_analysis['failed_queries'])}ê°œ")
    print()
    
    # ì‚¬ìš©ì í™œë™ ë¶„ì„
    print("ğŸ‘¥ ì‚¬ìš©ì í™œë™ ë¶„ì„:")
    user_analysis = analyzer.analyze_user_activity(hours)
    print(f"  - í™œì„± ì‚¬ìš©ì: {user_analysis['total_active_users']}ëª…")
    print(f"  - ì´ ë¡œê·¸ì¸: {user_analysis['total_logins']}íšŒ")
    print(f"  - ì´ ì±„íŒ… ë©”ì‹œì§€: {user_analysis['total_chat_messages']}ê°œ")
    print()
    
    if detailed:
        # ìƒì„¸ ì •ë³´ ì¶œë ¥
        print("ğŸ“‹ ìƒì„¸ ë¶„ì„:")
        print("-" * 30)
        
        if api_analysis['slow_requests']:
            print("â±ï¸ ëŠë¦° API ìš”ì²­:")
            for req in api_analysis['slow_requests'][:3]:
                print(f"  - {req['path']}: {req['response_time']:.0f}ms")
        
        if sql_analysis['slow_queries']:
            print("ğŸŒ ëŠë¦° SQL ì¿¼ë¦¬:")
            for query in sql_analysis['slow_queries'][:3]:
                print(f"  - {query['execution_time']:.3f}s: {query['query'][:50]}...")
        
        if error_analysis['error_details']:
            print("âŒ ìµœê·¼ ì—ëŸ¬:")
            for module, errors in error_analysis['error_details'].items():
                if errors:
                    latest_error = errors[0]
                    print(f"  - {module}: {latest_error['message'][:60]}...")

def tail_logs(log_type='app', lines=50):
    """ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§"""
    analyzer = LogAnalyzer()
    analyzer.tail_log(log_type, lines)

def generate_report(hours=24, output_file=None):
    """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
    analyzer = LogAnalyzer()
    
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"logs/report_{timestamp}.json"
    
    print(f"ğŸ“‹ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘: {output_file}")
    report = analyzer.generate_report(hours, output_file)
    
    print(f"âœ… ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
    return report

def cleanup_logs(days=30, dry_run=False):
    """ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬"""
    analyzer = LogAnalyzer()
    
    if dry_run:
        print(f"ğŸ” {days}ì¼ ì´ì „ ë¡œê·¸ íŒŒì¼ í™•ì¸ ì¤‘... (ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•ŠìŒ)")
        # TODO: dry-run ëª¨ë“œ êµ¬í˜„
    else:
        print(f"ğŸ§¹ {days}ì¼ ì´ì „ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        cleaned_files = analyzer.cleanup_old_logs(days)
        print(f"âœ… {len(cleaned_files)}ê°œ íŒŒì¼ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    parser = argparse.ArgumentParser(description='Text-to-SQL AI Agent ë¡œê·¸ ê´€ë¦¬ ë„êµ¬')
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´')
    
    # status ëª…ë ¹ì–´
    status_parser = subparsers.add_parser('status', help='ë¡œê·¸ íŒŒì¼ ìƒíƒœ í™•ì¸')
    
    # analyze ëª…ë ¹ì–´
    analyze_parser = subparsers.add_parser('analyze', help='ë¡œê·¸ ë¶„ì„')
    analyze_parser.add_argument('--hours', type=int, default=24, help='ë¶„ì„ ê¸°ê°„ (ì‹œê°„, ê¸°ë³¸ê°’: 24)')
    analyze_parser.add_argument('--detailed', action='store_true', help='ìƒì„¸ ë¶„ì„ ì •ë³´ í‘œì‹œ')
    
    # tail ëª…ë ¹ì–´
    tail_parser = subparsers.add_parser('tail', help='ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§')
    tail_parser.add_argument('--type', choices=['app', 'error', 'api', 'sql', 'chat', 'auth'],
                           default='app', help='ëª¨ë‹ˆí„°ë§í•  ë¡œê·¸ íƒ€ì…')
    tail_parser.add_argument('--lines', type=int, default=50, help='í‘œì‹œí•  ë¼ì¸ ìˆ˜')
    
    # report ëª…ë ¹ì–´
    report_parser = subparsers.add_parser('report', help='ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±')
    report_parser.add_argument('--hours', type=int, default=24, help='ë¶„ì„ ê¸°ê°„ (ì‹œê°„)')
    report_parser.add_argument('--output', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    
    # cleanup ëª…ë ¹ì–´
    cleanup_parser = subparsers.add_parser('cleanup', help='ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬')
    cleanup_parser.add_argument('--days', type=int, default=30, help='ë³´ê´€ ê¸°ê°„ (ì¼, ê¸°ë³¸ê°’: 30)')
    cleanup_parser.add_argument('--dry-run', action='store_true', help='ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  í™•ì¸ë§Œ')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    print("ğŸ¤– Text-to-SQL AI Agent ë¡œê·¸ ê´€ë¦¬ ë„êµ¬")
    print("=" * 50)
    
    try:
        if args.command == 'status':
            show_log_status()
        
        elif args.command == 'analyze':
            analyze_logs(args.hours, args.detailed)
        
        elif args.command == 'tail':
            tail_logs(args.type, args.lines)
        
        elif args.command == 'report':
            generate_report(args.hours, args.output)
        
        elif args.command == 'cleanup':
            cleanup_logs(args.days, args.dry_run)
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 